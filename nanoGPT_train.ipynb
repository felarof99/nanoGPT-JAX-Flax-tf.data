{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Mapping, Tuple\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jrand\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state  # Useful dataclass to keep train state\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import pdb\n",
        "import functools\n",
        "\n",
        "def println(*args):\n",
        "  for arg in args:\n",
        "    print(arg)\n"
      ],
      "metadata": {
        "id": "72Nj51EWBerM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "3zRBSLy46lIY",
        "outputId": "2110d978-39d2-4691-bb5d-b701edc7d783",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mnanoGPT-JAX-JAX-JAX\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "jBNxJaDJLWKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "713de7e5-2423-4189-8337-963d6553de4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[CpuDevice(id=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE_COUNT = len(jax.devices())\n",
        "DEVICE_COUNT"
      ],
      "metadata": {
        "id": "sqescrf0Sslj",
        "outputId": "13b2a8d2-91ca-44df-8313-12f5a4e05ac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHFANuKyilVM",
        "outputId": "f2006d95-797f-4173-c75f-ab5038f919ee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34massets\u001b[0m/  dataset.py  LICENSE   nanoGPT_JAX_JAX_JAX.ipynb  \u001b[01;34m__pycache__\u001b[0m/  trainer.py\n",
            "\u001b[01;34mdata\u001b[0m/    input.txt   model.py  nanoGPT_singe_file.ipynb   README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "\n",
        "import dataset\n",
        "import model\n",
        "\n",
        "importlib.reload(dataset)\n",
        "importlib.reload(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3Lwn46KOkxx",
        "outputId": "aa8ee106-8c51-40ce-bd1e-ebd86ca2e4b2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'model' from '/content/nanoGPT-JAX-JAX-JAX/model.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import chex\n",
        "from chex._src import fake\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "from dataclasses import dataclass\n",
        "import importlib\n",
        "import pdb\n",
        "\n",
        "# import dataset\n",
        "# import model\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "    key: jax.random.KeyArray\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    BATCH_SIZE: int = 256\n",
        "    BLOCK_SIZE: int = 64\n",
        "    T: int = 64\n",
        "    n_embed: int = 256\n",
        "    num_heads: int = 8\n",
        "    num_layers: int = 6\n",
        "\n",
        "config = Config()\n",
        "\n",
        "random_key = jax.random.PRNGKey(99)\n",
        "\n",
        "# Initialize model\n",
        "lm_model = model.LanguageModel(vocab_size=65,\n",
        "                      n_embed=config.n_embed,\n",
        "                      T=config.BLOCK_SIZE,\n",
        "                      num_heads=config.num_heads,\n",
        "                      num_layers=config.num_layers)\n",
        "sample_block_of_tokens = jnp.ones(shape=(config.T,), dtype=jnp.int32)\n",
        "output, params = lm_model.init_with_output(jax.random.PRNGKey(99), sample_block_of_tokens, training=False)\n",
        "params = params[\"params\"]\n",
        "\n",
        "def model_apply(params, inputs, training, dropout_key):\n",
        "    return lm_model.apply({\"params\": params}, inputs, training, rngs={'dropout': dropout_key})\n",
        "\n",
        "# Vectorize model apply function\n",
        "model_apply_batch = jax.vmap(model_apply, in_axes=(None, 0, None, None), out_axes=(0))\n",
        "\n",
        "PER_HOST_BATCH_SIZE = config.BATCH_SIZE // jax.device_count()\n",
        "\n",
        "# Define forward pass\n",
        "def forward_pass(params, state, batch, dropout_key):\n",
        "    inputs, targets = batch\n",
        "    logits = state.apply_fn(params, inputs, True, dropout_key)\n",
        "\n",
        "    chex.assert_shape(inputs, (PER_HOST_BATCH_SIZE, config.BLOCK_SIZE))\n",
        "    chex.assert_shape(targets, (PER_HOST_BATCH_SIZE, config.BLOCK_SIZE))\n",
        "\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
        "    loss = loss.mean()\n",
        "    return loss\n",
        "\n",
        "# Define training step\n",
        "def train_step(state, inputs, targets, dropout_key):\n",
        "    dropout_key = jax.random.fold_in(key=dropout_key, data=state.step)\n",
        "\n",
        "    batch = inputs, targets\n",
        "\n",
        "    grad_fn = jax.value_and_grad(forward_pass, argnums=(0))\n",
        "    loss, grads = grad_fn(state.params, state, batch, dropout_key)\n",
        "\n",
        "    loss = jax.lax.pmean(loss, axis_name=\"devices\")\n",
        "    grads = jax.lax.pmean(grads, axis_name=\"devices\")\n",
        "\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss\n",
        "\n",
        "# Initialize optimizer and training state\n",
        "opt = optax.adam(learning_rate=0.0001)\n",
        "state = TrainState.create(apply_fn=model_apply_batch, params=params, tx=opt, key=random_key)\n",
        "data = dataset.Dataset(batch_size=config.BATCH_SIZE, block_size=config.BLOCK_SIZE)\n",
        "\n",
        "# pmap the train_step.\n",
        "train_step_pmap = jax.jit(jax.pmap(train_step, in_axes=(0, 0, 0, None), out_axes=(0), axis_name=\"devices\"))\n",
        "states = jax.device_put_replicated(state, jax.local_devices())\n",
        "\n"
      ],
      "metadata": {
        "id": "92Md4ebTBeET",
        "outputId": "bb1da5f9-de7e-4505-bd0b-8db3a0805add",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/nanoGPT-JAX-JAX-JAX/dataset.py:49: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  data = jnp.array(_encode(text, self.stoi), dtype=jnp.int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run a training step\n",
        "# This is an **IMPURE function** for convenience. Don't JIT it.\n",
        "reload_libs = False\n",
        "if reload_libs:\n",
        "  importlib.reload(dataset)\n",
        "  importlib.reload(model)\n",
        "\n",
        "\n",
        "# fake_pmap = chex.fake_pmap_and_jit(enable_jit_patching=fake_jit, enable_pmap_patching=fake_pmap)\n",
        "# fake_pmap.start()\n",
        "num_epochs = 20\n",
        "steps_per_epoch = 1 # len(data.train_data) // config.BATCH_SIZE\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"epoch: \", epoch)\n",
        "  data.create_train_dataset()\n",
        "\n",
        "  for step in range(steps_per_epoch):\n",
        "    random_key, random_subkey = jax.random.split(random_key)\n",
        "\n",
        "    inputs, targets = data.get_batch()\n",
        "    # pdb.set_trace()\n",
        "\n",
        "    # create device dimension for minibatch\n",
        "    inputs = inputs.reshape((jax.device_count(), -1, inputs.shape[-1]))\n",
        "    targets = targets.reshape((jax.device_count(), -1, targets.shape[-1]))\n",
        "\n",
        "    # pdb.set_trace()\n",
        "\n",
        "    states, loss = train_step_pmap(states, inputs, targets, random_subkey)\n",
        "    print(\"loss\", loss[0], \"epoch\", epoch) if epoch % 1 == 0 else None\n",
        "\n",
        "# fake_pmap.stop()"
      ],
      "metadata": {
        "id": "4MVQEgvzBhFD",
        "outputId": "7cdb554a-d17c-450e-edc0-302139538dd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0\n",
            "loss 4.233783 epoch 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pmapping"
      ],
      "metadata": {
        "id": "REnxRDB6X1SB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify using flax multihead attention"
      ],
      "metadata": {
        "id": "Y0YxWb75OJlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_attention_outputs(custom_attention, flax_attention, input_shape, num_heads, head_size, rng_key):\n",
        "    # Create dummy input\n",
        "    x = jax.random.normal(rng_key, input_shape)\n",
        "\n",
        "    # Initialize custom attention\n",
        "    custom_params = custom_attention.init(rng_key, x, training=True)\n",
        "    custom_output = custom_attention.apply(custom_params, x, training=True, rngs={'dropout': rng_key})\n",
        "\n",
        "    # Initialize Flax attention\n",
        "    flax_params = flax_attention.init(rng_key, x, x, x)\n",
        "    flax_output = flax_attention.apply(flax_params, x, x, x)\n",
        "\n",
        "    print(\"custom_output: \", custom_output)\n",
        "    print(\"flax_output: \", flax_output)\n",
        "\n",
        "    # Compare outputs\n",
        "    return jnp.isclose(custom_output, flax_output, atol=1e-5).all()"
      ],
      "metadata": {
        "id": "9ZyBx0FtoHCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng_key = jax.random.PRNGKey(0)\n",
        "input_shape = (1, 2, 4)  # (batch_size, sequence_length, feature_size)\n",
        "num_heads = 4\n",
        "head_size = 16"
      ],
      "metadata": {
        "id": "oqRFw2VpoPOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom attention\n",
        "custom_attention = model.MultiHeadAttentionBatch(num_heads=num_heads, head_size=head_size, T=input_shape[1])\n",
        "\n",
        "# Flax attention\n",
        "flax_attention = nn.MultiHeadDotProductAttention(num_heads=num_heads, qkv_features=head_size * num_heads, out_features=head_size * num_heads)\n"
      ],
      "metadata": {
        "id": "XSbv5LgaoJBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = compare_attention_outputs(custom_attention, flax_attention, input_shape, num_heads, head_size, rng_key)\n",
        "print(\"Are the attention outputs close?\", result)"
      ],
      "metadata": {
        "id": "GXd_SnvQoc_R",
        "outputId": "88210b97-bdf2-4218-80db-2cf5410a75b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom_output:  [[[ 1.8267553   0.2545625   0.51664734  1.872377    0.\n",
            "   -0.24741195  0.06325258 -0.4732108   0.97036505  0.\n",
            "    1.2170275   1.5578686   1.0638489   0.          2.772833\n",
            "   -0.41109625  0.444793   -0.08247733  0.         -0.18067323\n",
            "    0.          0.          0.6723873  -0.93943655 -0.3522747\n",
            "    1.2153784  -3.7089698   1.3073872  -0.6657839  -0.5994085\n",
            "   -0.33070773 -1.8484493   0.37312767  0.44226554  0.60474485\n",
            "    2.2404766   0.         -1.8605132  -2.4844682  -0.56995404\n",
            "   -0.1442299   1.2074916  -0.11788648  2.850931    0.33974466\n",
            "    2.3744946  -2.746928    0.685969   -0.92724115 -1.0124649\n",
            "    0.          0.          1.3646483   0.4259958   1.1758763\n",
            "   -0.8295348   0.3146336   0.38039386 -1.96878    -1.0014266\n",
            "    0.88716567  1.783647    0.57467306  0.        ]\n",
            "  [ 0.00777232  0.8190602   2.6580398   1.651423   -0.9469865\n",
            "    0.48011455 -0.9287533   0.          0.         -2.8874931\n",
            "    0.60840005  2.0658875   0.35624415  0.          0.\n",
            "    0.70599437  0.5896166   1.2817444  -2.1008098  -0.04484057\n",
            "   -1.4275442   0.72321385  0.22759527  1.5051701   0.00882745\n",
            "    0.         -2.986158    0.5997018   0.7692054  -0.82710356\n",
            "   -0.1810503   0.          0.54273605  0.         -0.7080898\n",
            "    2.2388384   0.5093732  -0.23307195  0.          0.8846841\n",
            "   -0.43849385 -0.07484592  0.          2.513607    0.\n",
            "    1.1121328   0.5170551   2.5407615   0.          0.\n",
            "    0.83413124  0.57952935  0.4368621  -0.6022251   0.67910165\n",
            "    0.21055423  2.8181696   1.44193    -0.7840652   0.3328532\n",
            "    1.2413915   0.46679747  1.4598145   0.        ]]]\n",
            "flax_output:  [[[ 2.8895974e-02 -1.0068653e+00  1.2057748e+00  1.7973888e+00\n",
            "   -9.9489683e-01  1.1024659e+00 -3.2609969e-01  1.3099843e+00\n",
            "    3.7963891e-01 -6.6500354e-01 -2.2156236e+00  9.9098140e-01\n",
            "   -5.2030826e-01 -4.0043575e-01 -4.7597340e-01  6.1389470e-01\n",
            "   -5.1270604e-02  7.1665388e-01  3.6454529e-01  1.5936635e+00\n",
            "    2.7062860e-01 -2.4697018e+00  9.4948125e-01 -1.8499115e-01\n",
            "   -3.7676191e-01  1.8491125e-01 -2.2871125e-01 -1.4550415e+00\n",
            "   -3.6182284e-02  1.7561679e+00  5.6670117e-01  1.5644410e+00\n",
            "    2.3776472e-02  4.1427138e-01 -6.4342296e-01  3.9061794e-01\n",
            "   -2.4873943e+00  1.8754792e+00 -3.7327498e-01  5.1972145e-01\n",
            "    6.5622520e-01 -1.1356449e+00 -7.3058254e-01 -1.1278011e+00\n",
            "    3.8242584e-01 -5.6327289e-01 -1.0293421e+00 -3.2101333e-01\n",
            "    6.9993138e-03 -1.5518066e+00 -1.2681470e+00 -1.0305943e+00\n",
            "   -5.0882530e-01 -1.2005109e+00 -6.9858456e-01 -1.2420511e+00\n",
            "   -1.6567755e-01  3.1398809e-01  4.8796660e-01  5.3497815e-01\n",
            "   -1.6045654e+00 -5.6580853e-01 -1.4447480e-02 -2.5624591e-01]\n",
            "  [-3.1292903e-01 -2.8228211e-01  1.6164991e+00  1.1566637e+00\n",
            "   -1.7909843e-01  1.6738888e+00  1.9191897e-01  8.8188696e-01\n",
            "    3.4660101e-04 -6.1186695e-01 -1.4816654e+00  1.2603579e+00\n",
            "   -3.3277538e-01  7.8265357e-01 -8.8903624e-01  5.0843740e-01\n",
            "   -1.7042160e-03  4.8674318e-01  7.8109443e-01  2.0034916e+00\n",
            "    3.9721832e-01 -2.0879889e+00  6.5748179e-01 -2.9969342e-02\n",
            "   -5.1880574e-01  1.1667207e+00 -1.6108215e-01 -1.8761901e+00\n",
            "   -4.9229816e-01  2.1759119e+00  3.3186966e-01  1.3340666e+00\n",
            "    1.1307223e+00  3.8203084e-01  8.1939280e-02  2.2764298e-01\n",
            "   -2.6299930e+00  1.7664719e+00 -1.5143943e-01  7.9535151e-01\n",
            "    8.9765227e-01 -6.7262584e-01 -3.1963915e-01 -1.7064387e-01\n",
            "    8.3032340e-02 -8.6961055e-01 -8.8943326e-01 -1.3560627e+00\n",
            "   -1.2381132e+00 -8.6521959e-01 -1.4085336e+00 -1.1443344e+00\n",
            "   -3.0510187e-01 -1.2790594e+00 -1.1063838e-01 -8.6992276e-01\n",
            "   -3.7931356e-01  7.1076035e-02  2.8989238e-01 -5.1210642e-02\n",
            "   -1.1258061e+00 -7.6050615e-01  2.4350727e-01  2.7750367e-01]]]\n",
            "Are the attention outputs close? False\n"
          ]
        }
      ]
    }
  ]
}