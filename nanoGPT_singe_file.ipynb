{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YQxIzwuHA5ac",
        "outputId": "5df35aae-e3a3-44ae-9ade-a6ba6228e338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-24 18:16:53--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-02-24 18:16:53 (19.7 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "jax.devices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jBNxJaDJLWKG",
        "outputId": "b789bbbf-c73b-4cba-e074-7a153ec4090c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE_COUNT = len(jax.devices())\n",
        "DEVICE_COUNT"
      ],
      "metadata": {
        "id": "sqescrf0Sslj",
        "outputId": "a38b6e65-8475-4eda-99a2-f55f9c02f632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Mapping, Tuple\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jrand\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state  # Useful dataclass to keep train state\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import pdb\n",
        "import functools\n",
        "\n",
        "def println(*args):\n",
        "  for arg in args:\n",
        "    print(arg)\n"
      ],
      "metadata": {
        "id": "72Nj51EWBerM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below would result in a minibatch size of 32.\n",
        "BATCH_SIZE = 8 # how many independent sequences will we process in parallel?\n",
        "BLOCK_SIZE = 16 # what is the maximum context length for predictions?"
      ],
      "metadata": {
        "id": "6cW-0hUISacU"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create chars vocubulary using all the unique characters in the text.\n",
        "chars = sorted(list(set(text)))\n",
        "VOCAB_SIZE = len(chars)\n",
        "\n",
        "# Create mapping from characters to integers.\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "\n",
        "# Create reverse mapping from integers to characters.\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Create encode, decode function.\n",
        "def encode(s: str, stoi: Mapping[str, int]) -> List[int]:\n",
        "  return [stoi[c] for c in s]\n",
        "\n",
        "def decode(tokens: List[int], itos: Mapping[int, str]) -> str:\n",
        "  return ''.join([itos[i] for i in tokens])\n",
        "\n",
        "println(encode(\"hii there\", stoi), decode(encode(\"hii there\", stoi), itos))\n",
        "\n",
        "# Let's now split up the data into train and validation sets.\n",
        "data = jnp.array(encode(text, stoi), dtype=jnp.int64)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices(train_data)\n",
        "                .batch(BLOCK_SIZE+1)\n",
        "                .map(lambda input: (input[:BLOCK_SIZE], input[1:BLOCK_SIZE+1]),\n",
        "                     num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                .batch(BATCH_SIZE)\n",
        "                .repeat()\n",
        "                .as_numpy_iterator())\n",
        "val_dataset = (tf.data.Dataset.from_tensor_slices(val_data)\n",
        "                .batch(BLOCK_SIZE+1)\n",
        "                .map(lambda input: (input[:BLOCK_SIZE], input[1:BLOCK_SIZE+1]),\n",
        "                     num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                .batch(BATCH_SIZE)\n",
        "                .repeat()\n",
        "                .as_numpy_iterator())\n",
        "\n",
        "def get_batch(training: bool = True):\n",
        "  if not training:\n",
        "    val_batch = next(val_dataset)\n",
        "    return jnp.array(val_batch)\n",
        "\n",
        "  train_batch = next(train_dataset)\n",
        "  return jnp.array(train_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOS99UXYBjIZ",
        "outputId": "8537be1b-f7e3-4b24-dfa2-a3ce272ef675"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-108-6c607b0378a8>:24: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  data = jnp.array(encode(text, stoi), dtype=jnp.int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  output_size: int\n",
        "\n",
        "  def setup(self):\n",
        "    # **new**: attention paper uses 4 times token_info_size when doing linear transformation.\n",
        "    # and then projects it back to token_info_size in linear transformation layer.\n",
        "    self.ffwd = nn.Dense(features=4 * self.output_size)\n",
        "\n",
        "    # **new**: projection layer, which goes back into residual pathway.\n",
        "    self.projection = nn.Dense(self.output_size)\n",
        "\n",
        "  def __call__(self, x, training: bool):\n",
        "    x = nn.relu(self.ffwd(x))\n",
        "    x = self.projection(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "  token_info_size: int # head_size; how much (emb dim) info each token emits for keys, queries, values.\n",
        "  T: int # block size; number of tokens in a block\n",
        "\n",
        "  def setup(self):\n",
        "    # key, query will take vector of size C.\n",
        "    # i.e., channels containing info of token and will output token_info_size\n",
        "    self.key_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "    self.query_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "    self.value_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(rate=0.2)\n",
        "\n",
        "\n",
        "  def __call__(self, block_of_tokens_with_info_channels: jnp.array, training: bool):\n",
        "    \"\"\"Accepts a block of tokens with info channels, like (8, 65).\"\"\"\n",
        "\n",
        "    # TODO(ntnsonti): Double check; but tril should not be learnable according cGPT.\n",
        "    tril = jnp.tril(jnp.ones(shape=(self.T, self.T)))\n",
        "\n",
        "    # input: (T, info channels )\n",
        "    # output: (T, token_info_size)\n",
        "    keys = self.key_layer(block_of_tokens_with_info_channels)\n",
        "    queries = self.query_layer(block_of_tokens_with_info_channels)\n",
        "    values = self.value_layer(block_of_tokens_with_info_channels)\n",
        "\n",
        "    # chanel info size\n",
        "    C = int(block_of_tokens_with_info_channels.shape[-1])\n",
        "    # print(\"[ntn99] channel_info_size: \", C)\n",
        "\n",
        "    # compute attention score.\n",
        "    wei = jnp.dot(queries, keys.T) * C**0.5 # (T, token_info_size) * (token_info_size, T) == (T, T)\n",
        "    wei = jnp.where(tril==0, -jnp.inf, wei)\n",
        "    wei = nn.softmax(wei, axis=-1)\n",
        "\n",
        "    attention_values = jnp.dot(wei, values) # (T, T) * (T, token_info_size))\n",
        "\n",
        "    attention_values = self.dropout(attention_values, deterministic=not training)\n",
        "\n",
        "    return attention_values # (T, token_info_size)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  final_token_info_size: int # After concatenating from all heads, how much info (values -- emb size) you have on each token.\n",
        "  T: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.token_info_size_per_head = int(self.final_token_info_size/self.num_heads)\n",
        "    self.heads = [\n",
        "        Head(token_info_size=self.token_info_size_per_head, T=self.T) for _ in range(self.num_heads)\n",
        "    ]\n",
        "\n",
        "    self.projection = nn.Dense(features=self.final_token_info_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(rate=0.2)\n",
        "\n",
        "  def __call__(self, block_of_tokens_with_info_channels: jnp.array, training: bool):\n",
        "    out_from_each_head = jnp.array([h(block_of_tokens_with_info_channels, training) for h in self.heads])\n",
        "\n",
        "    # You just run multiple attention heads in parallel and concatenate\n",
        "    # their output along channel dimension, i.e., dim==-1\n",
        "    out_from_all_heads = jnp.concatenate(out_from_each_head, axis=-1)\n",
        "    # print(\"[ntn99] out_from_all_heads concatenated shape: \", out_from_all_heads.shape)\n",
        "\n",
        "    projection =  self.projection(out_from_all_heads)\n",
        "\n",
        "    return self.dropout(projection, deterministic=not training)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  num_heads: int\n",
        "  final_token_info_size: int\n",
        "  T: int\n",
        "\n",
        "  def setup(self):\n",
        "    # communication.\n",
        "    self.self_attention_heads = MultiHeadAttention(num_heads=self.num_heads,\n",
        "                                                   final_token_info_size=self.final_token_info_size,\n",
        "                                                   T=self.T)\n",
        "\n",
        "    # computation.\n",
        "    self.computation_layer = FeedForward(output_size=self.final_token_info_size)\n",
        "\n",
        "    self.ln1 = nn.LayerNorm()\n",
        "    self.ln2 = nn.LayerNorm()\n",
        "\n",
        "    self.dropout = nn.Dropout(rate=0.2)\n",
        "\n",
        "  def __call__(self, x, training: bool):\n",
        "    x = x + self.self_attention_heads(self.ln1(x), training)\n",
        "    # print(\"[ntn99] input size after attention_head: \", x.shape)\n",
        "\n",
        "    x = x + self.computation_layer(self.ln2(x), training)\n",
        "    # print(\"[ntn99] input size after computation (end of block): \", x.shape)\n",
        "\n",
        "    x = self.dropout(x, deterministic=not training)\n",
        "    return x\n",
        "\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "  \"\"\"Reads one char and predicits the next char.\"\"\"\n",
        "  vocab_size: int # number of vocabulary (number of rows of embedding table)\n",
        "  n_embed: int # embedding dim after lookup\n",
        "  T: int # block size, i.e., number of tokens attention block is looking at once\n",
        "\n",
        "  def setup(self):\n",
        "    # number of channels you want to use for store info for each token.\n",
        "    self.C = self.vocab_size\n",
        "\n",
        "    self.token_embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.n_embed)\n",
        "\n",
        "    self.pos_embedding_table = nn.Embed(num_embeddings=self.T, features=self.n_embed)\n",
        "\n",
        "    # *** new ***\n",
        "    # Since, there are 4 heads, each head only needs to output token_info of size 8.\n",
        "    # Concantenate token_info from all 4 heards, gives us 32\n",
        "    # self.self_attention_heads = MultiHeadAttention(num_heads=4, final_token_info_size=self.n_embed, T=self.T)\n",
        "    self.num_blocks = 4\n",
        "    self.blocks = [\n",
        "        Block(num_heads=4, final_token_info_size=self.n_embed, T=self.T) for _ in range(self.num_blocks)\n",
        "    ]\n",
        "    self.ln = nn.LayerNorm()\n",
        "    self.lang_model_head = nn.Dense(features=self.C)\n",
        "\n",
        "  def __call__(self, block_of_tokens: jnp.array, training: bool):\n",
        "    \"\"\"Accepts a block of tokens, like [0, 1, 2, 3, 4, 5, 6, 7].\"\"\"\n",
        "\n",
        "    # generate em for each token. output: (T, n_embed)\n",
        "    token_embs = self.token_embedding_table(block_of_tokens)\n",
        "\n",
        "    # generate position embs for each token.\n",
        "    ## get token positions.\n",
        "\n",
        "    # TODO(ntnsonti): setting num_pos to T always\n",
        "    # num_pos = block_of_tokens.shape[0]\n",
        "    num_pos = T\n",
        "    positions = jnp.arange(0, num_pos)\n",
        "    pos_embs = self.pos_embedding_table(positions)\n",
        "\n",
        "    # generate actual input to attention, x, which is sum of token_embs + pos_embs\n",
        "    x = token_embs + pos_embs\n",
        "\n",
        "    # feed x into self-attention head.\n",
        "    # x = self.self_attention_heads(x)\n",
        "    # x = self.blocks(x)(training)\n",
        "    for i in range(self.num_blocks):\n",
        "      x = self.blocks[i](x, training)\n",
        "\n",
        "    x = self.ln(x)\n",
        "\n",
        "    # generate logits for each token. output: (T, channels for info -- C)\n",
        "    token_logits = self.lang_model_head(x)\n",
        "\n",
        "    return token_logits\n"
      ],
      "metadata": {
        "id": "UUzSsxUPB5DK"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainState(train_state.TrainState):\n",
        "  key: jax.random.KeyArray\n",
        "\n",
        "T = BLOCK_SIZE\n",
        "random_key = jax.random.PRNGKey(99)\n",
        "random_key, random_subkey = jax.random.split(random_key)\n",
        "\n",
        "model = LanguageModel(vocab_size=65, n_embed=32, T=BLOCK_SIZE)\n",
        "\n",
        "# Now, our language model needs to accept a block of tokens, not one-char at a time.\n",
        "# We'll then make it accept a batch of blocks of tokens using vmap.\n",
        "sample_block_of_tokens = jnp.ones(shape=(T), dtype=jnp.int32)\n",
        "output, params = model.init_with_output(jrand.PRNGKey(99), sample_block_of_tokens, training=False)\n",
        "params = params[\"params\"]\n"
      ],
      "metadata": {
        "id": "3QM-Xtp1ZOcI"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_apply(params, inputs):\n",
        "  dropout_key = jax.random.PRNGKey(0) # TODO need to fix this.\n",
        "  return model.apply({\"params\": params}, inputs, False, rngs={'dropout': dropout_key})\n",
        "\n",
        "model_apply_batch = jax.vmap(model_apply, in_axes=(None, 0), out_axes=(0))\n",
        "\n",
        "def forward_pass(params, state, batch):\n",
        "  inputs, targets = batch\n",
        "  logits = state.apply_fn(params, inputs)\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
        "  loss = loss.mean()\n",
        "  return loss\n",
        "\n",
        "def train_step(state, batch):\n",
        "  grad_fn = jax.value_and_grad(forward_pass, argnums=(0))  # differentiate wrt 0th pos argument.\n",
        "  loss, grads = grad_fn(state.params, state, batch)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state, loss\n",
        "\n",
        "opt = optax.adam(learning_rate=0.0001)\n",
        "state = TrainState.create(apply_fn=model_apply_batch, params=params, tx=opt, key=random_key)"
      ],
      "metadata": {
        "id": "7e9M7_uLZ1VR"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1):\n",
        "  batch = get_batch()\n",
        "\n",
        "  random_key, random_subkey = jax.random.split(random_key)\n",
        "  dropout_key = jax.random.fold_in(key=random_key, data=state.step)\n",
        "\n",
        "  state, loss = train_step(state, batch)\n",
        "  print(\"loss\", loss, \"epoch\", epoch) if epoch%100==0 else None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFzsWcLnnw0X",
        "outputId": "3c820c8e-df7b-4798-eae0-52a40e274dd0"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 4.602925 epoch 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pmapping"
      ],
      "metadata": {
        "id": "REnxRDB6X1SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_apply(params, inputs):\n",
        "  dropout_key = jax.random.PRNGKey(0) # TODO need to fix this.\n",
        "  return model.apply({\"params\": params}, inputs, False, rngs={'dropout': dropout_key})\n",
        "\n",
        "model_apply_batch = jax.vmap(model_apply, in_axes=(None, 0), out_axes=(0))\n",
        "\n",
        "def forward_pass(params, state, batch):\n",
        "  inputs, targets = batch\n",
        "  logits = state.apply_fn(params, inputs)\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
        "  loss = loss.mean()\n",
        "  print(\"loss forward pass\", loss)\n",
        "  return loss\n",
        "\n",
        "def train_step(state, batch):\n",
        "  grad_fn = jax.value_and_grad(forward_pass, argnums=(0))  # differentiate wrt 0th pos argument.\n",
        "  loss, grads = grad_fn(state.params, state, batch)\n",
        "  print(\"loss before mean\", loss)\n",
        "\n",
        "  grads = jax.lax.pmean(grads, \"device\")\n",
        "  loss = jax.lax.pmean(loss, \"device\")\n",
        "\n",
        "  print(\"loss after mean\", loss)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state, loss"
      ],
      "metadata": {
        "id": "uBDpQdLOonWU"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = optax.adam(learning_rate=0.0001)\n",
        "state = TrainState.create(apply_fn=model_apply_batch, params=params, tx=opt, key=random_key)\n",
        "states = jax.device_put_replicated(state, jax.local_devices())"
      ],
      "metadata": {
        "id": "uQ48Hzj8ojqk"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with jax.disable_jit():\n",
        "  opt = optax.adam(learning_rate=0.0001)\n",
        "  state = TrainState.create(apply_fn=model_apply_batch, params=params, tx=opt, key=random_key)\n",
        "  states = jax.device_put_replicated(state, jax.local_devices())\n",
        "  train_step_pmap = jax.pmap(train_step, axis_name=\"device\")\n",
        "\n",
        "  for epoch in range(1):\n",
        "    inputs, targets = get_batch()\n",
        "    inputs = jnp.reshape(inputs, [DEVICE_COUNT, -1, inputs.shape[1]])\n",
        "    targets = jnp.reshape(targets, [DEVICE_COUNT, -1, targets.shape[1]])\n",
        "    batch = inputs, targets\n",
        "\n",
        "\n",
        "    states, loss = train_step_pmap(states, batch)\n",
        "    print(\"loss\", loss, \"epoch\", epoch) if epoch%100==0 else None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-vBXHpbocO9",
        "outputId": "ccece381-f39b-42bf-c46a-6e0707ee6a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss forward pass Traced<ShapedArray(float32[])>with<JVPTrace(level=2/1)> with\n",
            "  primal = Traced<ShapedArray(float32[])>with<MapTrace(level=0/1)> with\n",
            "    val = ShardedDeviceArray([4.256075 , 4.38717  , 4.1536055, 4.2484975, 4.3698626,\n",
            "                    4.6160975, 4.6228366, 3.9691005], dtype=float32)\n",
            "    shard_axes = {'device': 0}\n",
            "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=1/1)> with\n",
            "    pval = (ShapedArray(float32[]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7d2ad37c8450>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=1/1)>, Traced<ShapedArray(float32[]):JaxprTrace(level=1/1)>), out_tracer_refs=[<weakref at 0x7d2ad356f650; to 'JaxprTracer' at 0x7d2ad36c6890>], out_avals=[ShapedArray(float32[])], primitive=div, params={}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7d2ad3253db0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(state, batch):\n",
        "  random_key, random_subkey = jax.random.split(random_key)\n",
        "  dropout_key = jax.random.fold_in(random_key, data=state.step)\n",
        "\n",
        "  grad_fn = jax.value_and_grad(forward_pass, argnums=(0))  # differentiate wrt 0th pos argument.\n",
        "  loss, grads = grad_fn(state.params, state, batch, dropout_key)\n",
        "\n",
        "  return state, loss"
      ],
      "metadata": {
        "id": "kkbLNKwXUS-e"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y0YxWb75OJlX"
      },
      "execution_count": 116,
      "outputs": []
    }
  ]
}