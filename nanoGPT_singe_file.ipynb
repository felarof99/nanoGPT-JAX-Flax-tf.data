{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQxIzwuHA5ac",
        "outputId": "fb1a388c-2b80-4e2b-e0bc-49238d7ce2f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-13 18:33:37--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-03-13 18:33:37 (20.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "jax.devices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBNxJaDJLWKG",
        "outputId": "1e71ae69-7453-40aa-e7db-be6df4fee798"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE_COUNT = len(jax.devices())\n",
        "DEVICE_COUNT"
      ],
      "metadata": {
        "id": "sqescrf0Sslj",
        "outputId": "c6c28986-793f-48d2-fe24-d532083d4666",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Mapping, Tuple\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jrand\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state  # Useful dataclass to keep train state\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import pdb\n",
        "import functools\n",
        "\n",
        "def println(*args):\n",
        "  for arg in args:\n",
        "    print(arg)\n"
      ],
      "metadata": {
        "id": "72Nj51EWBerM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below would result in a minibatch size of 32.\n",
        "BATCH_SIZE = 8 # how many independent sequences will we process in parallel?\n",
        "BLOCK_SIZE = 16 # what is the maximum context length for predictions?"
      ],
      "metadata": {
        "id": "6cW-0hUISacU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create chars vocubulary using all the unique characters in the text.\n",
        "chars = sorted(list(set(text)))\n",
        "VOCAB_SIZE = len(chars)\n",
        "\n",
        "# Create mapping from characters to integers.\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "\n",
        "# Create reverse mapping from integers to characters.\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Create encode, decode function.\n",
        "def encode(s: str, stoi: Mapping[str, int]) -> List[int]:\n",
        "  return [stoi[c] for c in s]\n",
        "\n",
        "def decode(tokens: List[int], itos: Mapping[int, str]) -> str:\n",
        "  return ''.join([itos[i] for i in tokens])\n",
        "\n",
        "println(encode(\"hii there\", stoi), decode(encode(\"hii there\", stoi), itos))\n",
        "\n",
        "# Let's now split up the data into train and validation sets.\n",
        "data = jnp.array(encode(text, stoi), dtype=jnp.int64)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices(train_data)\n",
        "                .batch(BLOCK_SIZE+1)\n",
        "                .map(lambda input: (input[:BLOCK_SIZE], input[1:BLOCK_SIZE+1]),\n",
        "                     num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                .batch(BATCH_SIZE)\n",
        "                .repeat()\n",
        "                .as_numpy_iterator())\n",
        "val_dataset = (tf.data.Dataset.from_tensor_slices(val_data)\n",
        "                .batch(BLOCK_SIZE+1)\n",
        "                .map(lambda input: (input[:BLOCK_SIZE], input[1:BLOCK_SIZE+1]),\n",
        "                     num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                .batch(BATCH_SIZE)\n",
        "                .repeat()\n",
        "                .as_numpy_iterator())\n",
        "\n",
        "def get_batch(training: bool = True):\n",
        "  if not training:\n",
        "    val_batch = next(val_dataset)\n",
        "    return jnp.array(val_batch)\n",
        "\n",
        "  train_batch = next(train_dataset)\n",
        "  return jnp.array(train_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOS99UXYBjIZ",
        "outputId": "dd517798-ec71-496d-f616-322d4667c77c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-6c607b0378a8>:24: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  data = jnp.array(encode(text, stoi), dtype=jnp.int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleExampleMultiHeadSelfAttention(nn.Module):\n",
        "    num_heads: int\n",
        "    projection_dim: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        # x shape: (seq_len, projection_dim)\n",
        "        query = nn.Dense(self.projection_dim)(x)\n",
        "        key = nn.Dense(self.projection_dim)(x)\n",
        "        value = nn.Dense(self.projection_dim)(x)\n",
        "\n",
        "        query = query.reshape((self.num_heads, self.projection_dim // self.num_heads))\n",
        "        key = key.reshape((self.num_heads, self.projection_dim // self.num_heads))\n",
        "        value = value.reshape((self.num_heads, self.projection_dim // self.num_heads))\n",
        "\n",
        "        attention_scores = jnp.einsum('hd,Hd->hH', query, key) / jnp.sqrt(self.projection_dim)\n",
        "        attention_weights = nn.softmax(attention_scores, axis=-1)\n",
        "        attention_output = jnp.einsum('hH,Hd->hd', attention_weights, value)\n",
        "        attention_output = attention_output.reshape((self.num_heads * (self.projection_dim // self.num_heads)))\n",
        "\n",
        "        output = nn.Dense(self.projection_dim)(attention_output)\n",
        "        return output\n",
        "\n",
        "# Vectorize the single-example module to handle batches\n",
        "# batch_multi_head_self_attention = jax.vmap(SingleExampleMultiHeadSelfAttention(num_heads=8, projection_dim=64), in_axes=0, out_axes=0)"
      ],
      "metadata": {
        "id": "njCIqlWvPHzA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadAttention(nn.Module):\n",
        "  head_size: int # token_info_size; how much (emb dim) info each token emits for keys, queries, values.\n",
        "  T: int # block size; number of tokens in a block\n",
        "\n",
        "  def setup(self):\n",
        "    # key, query will take vector of size C.\n",
        "    # i.e., channels containing info of token and will output head_size\n",
        "    self.key_layer = nn.Dense(self.head_size, use_bias=False)\n",
        "    self.query_layer = nn.Dense(self.head_size, use_bias=False)\n",
        "    self.value_layer = nn.Dense(self.head_size, use_bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(rate=0.2)\n",
        "\n",
        "\n",
        "\n",
        "  def __call__(self, block_of_tokens_with_info_channels: jnp.array, training: bool):\n",
        "    \"\"\"Accepts a block of tokens with info channels, like (8, 65).\"\"\"\n",
        "    # TODO(ntnsonti): Double check; but tril should not be learnable according cGPT.\n",
        "    tril = jnp.tril(jnp.ones(shape=(self.T, self.T)))\n",
        "\n",
        "    # input: (T, info channels)\n",
        "    # output: (T, head_size)\n",
        "    keys = self.key_layer(block_of_tokens_with_info_channels)\n",
        "    queries = self.query_layer(block_of_tokens_with_info_channels)\n",
        "    values = self.value_layer(block_of_tokens_with_info_channels)\n",
        "\n",
        "    # chanel info size\n",
        "    C = int(block_of_tokens_with_info_channels.shape[-1])\n",
        "\n",
        "    # compute attention score.\n",
        "    wei = jnp.dot(queries, keys.T) * C**0.5 # (T, head_size) * (head_size, T) == (T, T)\n",
        "    wei = jnp.where(tril==0, -jnp.inf, wei)\n",
        "    wei = nn.softmax(wei, axis=-1)\n",
        "\n",
        "    attention_values = jnp.dot(wei, values) # (T, T) * (T, head_size))\n",
        "\n",
        "    attention_values = self.dropout(attention_values, deterministic=not training)\n",
        "\n",
        "    return attention_values # (T, head_size)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  head_size: int # head_size * num_heads is the final embedding dimension you get, after concatenating from all heads\n",
        "  T: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.heads = [\n",
        "        SingleHeadAttention(head_size=self.head_size, T=self.T) for _ in range(self.num_heads)\n",
        "    ]\n",
        "\n",
        "    final_output_size = self.num_heads * self.head_size\n",
        "    self.projection = nn.Dense(features=final_output_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(rate=0.2)\n",
        "\n",
        "  def __call__(self, block_of_tokens_with_info_channels: jnp.array, training: bool):\n",
        "    out_from_each_head = jnp.array([h(block_of_tokens_with_info_channels, training) for h in self.heads])\n",
        "\n",
        "    # You just run multiple attention heads in parallel and concatenate\n",
        "    # their output along channel dimension, i.e., dim==-1\n",
        "    out_from_all_heads = jnp.concatenate(out_from_each_head, axis=-1)\n",
        "    # print(\"[ntn99] out_from_all_heads concatenated shape: \", out_from_all_heads.shape)\n",
        "\n",
        "    projection =  self.projection(out_from_all_heads)\n",
        "\n",
        "    return self.dropout(projection, deterministic=not training)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  output_size: int\n",
        "\n",
        "  def setup(self):\n",
        "    # Attention paper uses 4 times token_info_size when doing linear transformation\n",
        "    # and then projects it back to token_info_size in linear transformation layer.\n",
        "    self.ffwd = nn.Dense(features=4 * self.output_size)\n",
        "    self.projection = nn.Dense(self.output_size)\n",
        "\n",
        "  def __call__(self, x, training: bool):\n",
        "    x = nn.relu(self.ffwd(x))\n",
        "    x = self.projection(x)\n",
        "    return x\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "  num_heads: int\n",
        "  output_size: int # final_token_info_size; head_size * num_heads is the final embedding dimension you get, after concatenating from all heads\n",
        "  T: int\n",
        "\n",
        "  def setup(self):\n",
        "    # communication.\n",
        "    self.head_size = self.output_size // self.num_heads  # each SingleAttentionHead will produce head_size worth of info for key, value, querie. You concatenate all of them to get the final n_embed.\n",
        "    self.self_attention_heads = MultiHeadAttention(num_heads=self.num_heads,\n",
        "                                                   head_size = self.head_size,\n",
        "                                                   T=self.T)\n",
        "\n",
        "    # computation.\n",
        "    self.computation_layer = FeedForward(output_size=self.output_size)\n",
        "\n",
        "    self.ln1 = nn.LayerNorm()\n",
        "    self.ln2 = nn.LayerNorm()\n",
        "\n",
        "    self.dropout = nn.Dropout(rate=0.2)\n",
        "\n",
        "  def __call__(self, x, training: bool):\n",
        "    x = x + self.self_attention_heads(self.ln1(x), training)\n",
        "\n",
        "    x = x + self.computation_layer(self.ln2(x), training)\n",
        "\n",
        "    x = self.dropout(x, deterministic=not training)\n",
        "    return x"
      ],
      "metadata": {
        "id": "UUzSsxUPB5DK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  \"\"\"Reads one char and predicits the next char.\"\"\"\n",
        "  vocab_size: int # number of vocabulary (number of rows of embedding table)\n",
        "  n_embed: int # embedding dim after lookup\n",
        "  T: int # block size, i.e., number of tokens attention block is looking at once\n",
        "\n",
        "  def setup(self):\n",
        "    # number of channels you want to use for store info for each token.\n",
        "    self.C = self.vocab_size\n",
        "\n",
        "    self.token_embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.n_embed)\n",
        "\n",
        "    self.pos_embedding_table = nn.Embed(num_embeddings=self.T, features=self.n_embed)\n",
        "\n",
        "    # Since, there are 4 heads, each head only needs to output token_info of size 8.\n",
        "    # Concantenate token_info from all 4 heards, gives us 32\n",
        "    self.num_blocks = 4\n",
        "    self.blocks = [\n",
        "        TransformerEncoderBlock(num_heads=4,\n",
        "                                output_size=self.n_embed,\n",
        "                                T=self.T) for _ in range(self.num_blocks)\n",
        "    ]\n",
        "    self.ln = nn.LayerNorm()\n",
        "    self.lang_model_head = nn.Dense(features=self.C)\n",
        "\n",
        "  def __call__(self, block_of_tokens: jnp.array, training: bool):\n",
        "    \"\"\"Accepts a block of tokens, like [0, 1, 2, 3, 4, 5, 6, 7].\"\"\"\n",
        "    # generate emb for each token. output: (T, n_embed)\n",
        "    token_embs = self.token_embedding_table(block_of_tokens)\n",
        "\n",
        "    # generate position embs for each token.\n",
        "    # get token positions.\n",
        "\n",
        "    # num_pos = block_of_tokens.shape[0]\n",
        "    num_pos = T\n",
        "    positions = jnp.arange(0, num_pos)\n",
        "    pos_embs = self.pos_embedding_table(positions)\n",
        "\n",
        "    # generate actual input to attention, x, which is sum of token_embs + pos_embs\n",
        "    x = token_embs + pos_embs\n",
        "\n",
        "    # feed x into self-attention head.\n",
        "    # x = self.self_attention_heads(x)\n",
        "    # x = self.blocks(x)(training)\n",
        "    for i in range(self.num_blocks):\n",
        "      x = self.blocks[i](x, training)\n",
        "\n",
        "    x = self.ln(x)\n",
        "\n",
        "    # generate logits for each token. output: (T, channels for info -- C)\n",
        "    token_logits = self.lang_model_head(x)\n",
        "\n",
        "    return token_logits\n"
      ],
      "metadata": {
        "id": "WOkGMx7VIknF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainState(train_state.TrainState):\n",
        "  key: jax.random.KeyArray\n",
        "\n",
        "T = BLOCK_SIZE\n",
        "random_key = jax.random.PRNGKey(99)\n",
        "random_key, random_subkey = jax.random.split(random_key)\n",
        "\n",
        "model = LanguageModel(vocab_size=65, n_embed=32, T=BLOCK_SIZE)\n",
        "\n",
        "# Now, our language model needs to accept a block of tokens, not one-char at a time.\n",
        "# We'll then make it accept a batch of blocks of tokens using vmap.\n",
        "sample_block_of_tokens = jnp.ones(shape=(T), dtype=jnp.int32)\n",
        "output, params = model.init_with_output(jrand.PRNGKey(99), sample_block_of_tokens, training=False)\n",
        "params = params[\"params\"]\n"
      ],
      "metadata": {
        "id": "3QM-Xtp1ZOcI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_apply(params, inputs):\n",
        "  dropout_key = jax.random.PRNGKey(0) # TODO need to fix this.\n",
        "  return model.apply({\"params\": params}, inputs, False, rngs={'dropout': dropout_key})\n",
        "\n",
        "model_apply_batch = jax.vmap(model_apply, in_axes=(None, 0), out_axes=(0))\n",
        "\n",
        "def forward_pass(params, state, batch):\n",
        "  inputs, targets = batch\n",
        "  logits = state.apply_fn(params, inputs)\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
        "  loss = loss.mean()\n",
        "  return loss\n",
        "\n",
        "def train_step(state, batch):\n",
        "  grad_fn = jax.value_and_grad(forward_pass, argnums=(0))  # differentiate wrt 0th pos argument.\n",
        "  loss, grads = grad_fn(state.params, state, batch)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state, loss\n",
        "\n",
        "opt = optax.adam(learning_rate=0.0001)\n",
        "state = TrainState.create(apply_fn=model_apply_batch, params=params, tx=opt, key=random_key)"
      ],
      "metadata": {
        "id": "7e9M7_uLZ1VR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1):\n",
        "  batch = get_batch()\n",
        "\n",
        "  random_key, random_subkey = jax.random.split(random_key)\n",
        "  dropout_key = jax.random.fold_in(key=random_key, data=state.step)\n",
        "\n",
        "  state, loss = train_step(state, batch)\n",
        "  print(\"loss\", loss, \"epoch\", epoch) if epoch%100==0 else None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFzsWcLnnw0X",
        "outputId": "eb8a2e6a-2bc0-4c08-8ab6-468ba6b10850"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 4.602925 epoch 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pmapping"
      ],
      "metadata": {
        "id": "REnxRDB6X1SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_apply(params, inputs):\n",
        "  dropout_key = jax.random.PRNGKey(0) # TODO need to fix this.\n",
        "  return model.apply({\"params\": params}, inputs, False, rngs={'dropout': dropout_key})\n",
        "\n",
        "def forward_pass(params, state, batch):\n",
        "  inputs, targets = batch\n",
        "  logits = state.apply_fn(params, inputs)\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
        "  loss = loss.mean()\n",
        "  print(\"forward pass loss 1\", loss)\n",
        "  loss = jax.lax.pmean(loss, \"device\")\n",
        "  print(\"forward pass loss 2\", loss)\n",
        "  return loss\n",
        "\n",
        "def train_step(state, batch):\n",
        "  grad_fn = jax.value_and_grad(forward_pass, argnums=(0))  # differentiate wrt 0th pos argument.\n",
        "  loss, grads = grad_fn(state.params, state, batch)\n",
        "  print(\"loss before mean\", loss)\n",
        "\n",
        "  grads = jax.lax.pmean(grads, \"device\")\n",
        "  # loss = jax.lax.pmean(loss, \"device\")\n",
        "\n",
        "  print(\"loss after mean\", loss)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state, loss"
      ],
      "metadata": {
        "id": "uBDpQdLOonWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = optax.adam(learning_rate=0.0001)\n",
        "state = TrainState.create(apply_fn=model_apply_batch, params=params, tx=opt, key=random_key)\n",
        "states = jax.device_put_replicated(state, jax.local_devices())"
      ],
      "metadata": {
        "id": "uQ48Hzj8ojqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with jax.disable_jit():\n",
        "  model_apply_batch = jax.vmap(model_apply, in_axes=(None, 0), out_axes=(0))\n",
        "\n",
        "  opt = optax.adam(learning_rate=0.0001)\n",
        "  state = TrainState.create(apply_fn=model_apply_batch, params=params, tx=opt, key=random_key)\n",
        "  states = jax.device_put_replicated(state, jax.local_devices())\n",
        "  train_step_pmap = jax.pmap(train_step, axis_name=\"device\")\n",
        "\n",
        "  for epoch in range(1):\n",
        "    inputs, targets = get_batch()\n",
        "    inputs = jnp.reshape(inputs, [DEVICE_COUNT, -1, inputs.shape[1]])\n",
        "    targets = jnp.reshape(targets, [DEVICE_COUNT, -1, targets.shape[1]])\n",
        "    batch = inputs, targets\n",
        "\n",
        "\n",
        "    states, loss = train_step_pmap(states, batch)\n",
        "    print(\"loss\", loss, \"epoch\", epoch) if epoch%100==0 else None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-vBXHpbocO9",
        "outputId": "005fb748-1a94-4890-ebe3-335a778f00b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forward pass loss 1 Traced<ShapedArray(float32[])>with<JVPTrace(level=2/1)> with\n",
            "  primal = Traced<ShapedArray(float32[])>with<MapTrace(level=0/1)> with\n",
            "    val = ShardedDeviceArray([4.4624095, 4.098372 , 4.7049522, 4.719946 , 4.3354793,\n",
            "                    4.6712008, 4.5151587, 4.525096 ], dtype=float32)\n",
            "    shard_axes = {'device': 0}\n",
            "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=1/1)> with\n",
            "    pval = (ShapedArray(float32[]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7842309a7cb0>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=1/1)>, Traced<ShapedArray(float32[]):JaxprTrace(level=1/1)>), out_tracer_refs=[<weakref at 0x784230494a90; to 'JaxprTracer' at 0x784230496930>], out_avals=[ShapedArray(float32[])], primitive=div, params={}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7842306662f0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
            "forward pass loss 2 Traced<ShapedArray(float32[])>with<JVPTrace(level=2/1)> with\n",
            "  primal = Traced<ShapedArray(float32[])>with<MapTrace(level=0/1)> with\n",
            "    val = ShardedDeviceArray([4.504077, 4.504077, 4.504077, 4.504077, 4.504077,\n",
            "                    4.504077, 4.504077, 4.504077], dtype=float32)\n",
            "    shard_axes = {'device': 0}\n",
            "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=1/1)> with\n",
            "    pval = (ShapedArray(float32[]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7842309a58b0>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=1/1)>, Traced<ShapedArray(float32[]):JaxprTrace(level=1/1)>), out_tracer_refs=[<weakref at 0x7842304968e0; to 'JaxprTracer' at 0x784230494d10>], out_avals=[ShapedArray(float32[])], primitive=div, params={}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x784230a2ce30>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
            "loss before mean [4.504077 4.504077 4.504077 4.504077 4.504077 4.504077 4.504077 4.504077]{device=0}\n",
            "loss after mean [4.504077 4.504077 4.504077 4.504077 4.504077 4.504077 4.504077 4.504077]{device=0}\n",
            "loss [4.504077 4.504077 4.504077 4.504077 4.504077 4.504077 4.504077 4.504077] epoch 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(state, batch):\n",
        "  random_key, random_subkey = jax.random.split(random_key)\n",
        "  dropout_key = jax.random.fold_in(random_key, data=state.step)\n",
        "\n",
        "  grad_fn = jax.value_and_grad(forward_pass, argnums=(0))  # differentiate wrt 0th pos argument.\n",
        "  loss, grads = grad_fn(state.params, state, batch, dropout_key)\n",
        "\n",
        "  return state, loss"
      ],
      "metadata": {
        "id": "kkbLNKwXUS-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y0YxWb75OJlX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}