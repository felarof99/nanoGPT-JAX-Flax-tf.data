{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Mapping, Tuple\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jrand\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state  # Useful dataclass to keep train state\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import pdb\n",
        "import functools\n",
        "\n",
        "def println(*args):\n",
        "  for arg in args:\n",
        "    print(arg)\n"
      ],
      "metadata": {
        "id": "72Nj51EWBerM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "jBNxJaDJLWKG",
        "outputId": "3d47e23f-5372-4447-f7ae-8d6c350aa4e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE_COUNT = len(jax.devices())\n",
        "DEVICE_COUNT"
      ],
      "metadata": {
        "id": "sqescrf0Sslj",
        "outputId": "d6fa4e92-bef6-420d-b503-96013584227e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "nwtZXHlFGr59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Mapping, Tuple\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import tensorflow as tf\n",
        "import requests\n",
        "\n",
        "# Below would result in a minibatch size of 32.\n",
        "BATCH_SIZE = 32 # how many independent sequences will we process in parallel?\n",
        "BLOCK_SIZE = 16 # what is the maximum context length for predictions?\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create chars vocubulary using all the unique characters in the text.\n",
        "chars = sorted(list(set(text)))\n",
        "VOCAB_SIZE = len(chars)\n",
        "\n",
        "# Create mapping from characters to integers.\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "\n",
        "# Create reverse mapping from integers to characters.\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Create encode, decode function.\n",
        "def encode(s: str, stoi: Mapping[str, int]) -> List[int]:\n",
        "  return [stoi[c] for c in s]\n",
        "\n",
        "def decode(tokens: List[int], itos: Mapping[int, str]) -> str:\n",
        "  return ''.join([itos[i] for i in tokens])\n",
        "\n",
        "println(encode(\"hii there\", stoi), decode(encode(\"hii there\", stoi), itos))\n",
        "\n",
        "# Let's now split up the data into train and validation sets.\n",
        "data = jnp.array(encode(text, stoi), dtype=jnp.int64)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Below would result in a minibatch size of 32.\n",
        "BATCH_SIZE = 4 # how many independent sequences will we process in parallel?\n",
        "BLOCK_SIZE = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "\n",
        "def create_dataset(training: bool = True):\n",
        "  data = train_data if training else val_data\n",
        "  dataset = (tf.data.Dataset.from_tensor_slices(data)\n",
        "                .batch(BLOCK_SIZE+1)\n",
        "                .map(lambda input: (input[:BLOCK_SIZE], input[1:BLOCK_SIZE+1]),\n",
        "                     num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                .batch(BATCH_SIZE)\n",
        "                .repeat()\n",
        "                .as_numpy_iterator())\n",
        "  return dataset\n",
        "\n",
        "def get_batch(dataset):\n",
        "  batch = next(dataset)\n",
        "  return jnp.array(batch)\n",
        "\n",
        "train_dataset = create_dataset(training=True)\n",
        "val_dataset = create_dataset(training=False)"
      ],
      "metadata": {
        "id": "26cmngjtGo3T",
        "outputId": "14324c71-4904-4de2-95fd-cd783533fff3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-24 03:53:24--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-04-24 03:53:24 (20.1 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n",
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-e3499656a465>:38: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  data = jnp.array(encode(text, stoi), dtype=jnp.int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test dataset"
      ],
      "metadata": {
        "id": "6xNHrv8UIsbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch(train_dataset)\n",
        "println(\"inputs\", xb, \"inputs shape\", xb.shape)\n",
        "println(\"targets\", yb, \"targets shape\", yb.shape)\n",
        "for b in range(BATCH_SIZE): # batch dimension\n",
        "    for t in range(BLOCK_SIZE): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "id": "Js5MiGZ2IuVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "TYcDwpPDJn52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Mapping, Tuple\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jrand\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state  # Useful dataclass to keep train state\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import pdb\n",
        "import functools\n",
        "\n",
        "class SingleHeadAttention(nn.Module):\n",
        "  \"\"\"Implements a simple SingleHeadAttention layer for a single example from batch.\"\"\"\n",
        "\n",
        "  head_size: int\n",
        "  num_tokens: int\n",
        "  dropout_rate: float = 0.2\n",
        "  use_causal_mask: bool = False  # if True, only attend to past tokens.\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, tokens: jnp.array, training: bool):\n",
        "    \"\"\"Tokens, each with some channel dim.\"\"\"\n",
        "    # Use separate single dense layers for calculating keys, query, values\n",
        "    keys = nn.Dense(self.head_size, use_bias=False)(tokens)\n",
        "    queries = nn.Dense(self.head_size, use_bias=False)(tokens)\n",
        "    values = nn.Dense(self.head_size, use_bias=False)(tokens)\n",
        "\n",
        "    mask = (\n",
        "        jnp.tril(jnp.ones(shape=(self.num_tokens, self.num_tokens)))\n",
        "        if self.use_causal_mask\n",
        "        else jnp.ones(shape=(self.num_tokens, self.num_tokens))\n",
        "    )\n",
        "\n",
        "    # compute attention score.\n",
        "    wei = jnp.dot(queries, keys.T) / jnp.sqrt(self.head_size)\n",
        "    wei = jnp.where(mask == 0, -jnp.inf, wei)\n",
        "    wei = nn.softmax(wei, axis=-1)\n",
        "\n",
        "    attention_values = jnp.dot(\n",
        "        wei, values\n",
        "    )  # (num_tokens, num_tokens) * (num_tokens, head_size)\n",
        "    attention_values = nn.Dropout(rate=self.dropout_rate, deterministic=True)(\n",
        "        attention_values\n",
        "    )\n",
        "    return attention_values  # (num_tokens, head_size)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\"Implements a MultiHeadAttention layer for a single example from batch.\"\"\"\n",
        "\n",
        "  head_size: int\n",
        "  num_heads: int\n",
        "  num_tokens: int\n",
        "  dropout_rate: float = 0.2\n",
        "\n",
        "  def setup(self):\n",
        "    self.heads = [\n",
        "        SingleHeadAttention(\n",
        "            head_size=self.head_size, num_tokens=self.num_tokens\n",
        "        )\n",
        "        for _ in range(self.num_heads)\n",
        "    ]\n",
        "\n",
        "    # Project concatenated output from all attention heads to final output\n",
        "    # dimension, which is head_size * num_heads.\n",
        "    self.projection = nn.Dense(features=self.num_heads * self.head_size)\n",
        "    self.dropout = nn.Dropout(rate=self.dropout_rate, deterministic=True)\n",
        "\n",
        "  def __call__(self, tokens: jnp.array, training: bool):\n",
        "    output_from_each_head = []\n",
        "    for h in self.heads:\n",
        "      head_output = h(tokens, training)\n",
        "      output_from_each_head.append(head_output)\n",
        "\n",
        "    # Run multiple attention heads in parallel and concatenate\n",
        "    # their output along channel dimension, i.e., dim==-1\n",
        "    out_from_all_heads = jnp.concatenate(output_from_each_head, axis=-1)\n",
        "\n",
        "    projection = self.projection(out_from_all_heads)\n",
        "    return self.dropout(projection)\n",
        "\n",
        "\n",
        "class MultiHeadAttentionBatch(nn.Module):\n",
        "  \"\"\"Extends MultiHeadAttention to work on a batch of data.\"\"\"\n",
        "\n",
        "  head_size: int\n",
        "  num_heads: int\n",
        "  num_tokens: int\n",
        "  dropout_rate: float = 0.2\n",
        "\n",
        "  def setup(self):\n",
        "    self.multi_head_attn_single_example = MultiHeadAttention(\n",
        "        num_heads=self.num_heads,\n",
        "        head_size=self.head_size,\n",
        "        num_tokens=self.num_tokens,\n",
        "        dropout_rate=self.dropout_rate,\n",
        "    )\n",
        "\n",
        "    self.multi_head_attn_batch = jax.vmap(\n",
        "        self.multi_head_attn_single_example,\n",
        "        in_axes=(0, None),  # tokens, training\n",
        "        out_axes=(0),\n",
        "    )\n",
        "\n",
        "  def __call__(self, tokens: jnp.array, training: bool):\n",
        "    return self.multi_head_attn_batch(tokens, training)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  output_size: int\n",
        "\n",
        "  def setup(self):\n",
        "    # Attention paper uses 4 times token_info_size when doing linear transformation\n",
        "    # and then projects it back to token_info_size in linear transformation layer.\n",
        "    self.ffwd = nn.Dense(features=4 * self.output_size)\n",
        "    self.projection = nn.Dense(self.output_size)\n",
        "\n",
        "  def __call__(self, x, training: bool):\n",
        "    x = nn.relu(self.ffwd(x))\n",
        "    x = self.projection(x)\n",
        "    return x\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "  num_heads: int\n",
        "  # output_size = head_size * num_heads, is the final embedding dimension you get after concatenating from all heads.\n",
        "  output_size: int\n",
        "  num_tokens: int\n",
        "\n",
        "  def setup(self):\n",
        "    # communication.\n",
        "    # each single head will produce head_size worth of info for key, value, querie. You concatenate all of them to get the final output_size.\n",
        "    self.head_size = self.output_size // self.num_heads\n",
        "    self.self_attention_heads = MultiHeadAttention(num_heads=self.num_heads,\n",
        "                                                   head_size = self.head_size,\n",
        "                                                   num_tokens=self.num_tokens)\n",
        "\n",
        "    # computation.\n",
        "    self.computation_layer = FeedForward(output_size=self.output_size)\n",
        "\n",
        "    self.ln1 = nn.LayerNorm()\n",
        "    self.ln2 = nn.LayerNorm()\n",
        "\n",
        "    self.dropout = nn.Dropout(rate=0.2, deterministic=True)\n",
        "\n",
        "  def __call__(self, x, training: bool):\n",
        "    # transformer encoder forward pass\n",
        "    x = x + self.self_attention_heads(self.ln1(x), training)\n",
        "\n",
        "    x = x + self.computation_layer(self.ln2(x), training)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "  \"\"\"Reads one char and predicits the next char.\"\"\"\n",
        "  vocab_size: int # number of vocabulary (number of rows of embedding table)\n",
        "  n_embed: int # embedding dim after lookup\n",
        "  num_tokens: int # block size, i.e., number of tokens attention block is looking at once\n",
        "  num_heads: int\n",
        "  num_layers: int\n",
        "\n",
        "  def setup(self):\n",
        "    # number of channels you want to use for store info for each token.\n",
        "    self.C = self.vocab_size\n",
        "\n",
        "    self.token_embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.n_embed)\n",
        "\n",
        "    self.pos_embedding_table = nn.Embed(num_embeddings=self.num_tokens, features=self.n_embed)\n",
        "\n",
        "    # Since, there are 4 heads, each head only needs to output token_info of size 8.\n",
        "    # Concantenate token_info from all 4 heards, gives us 32\n",
        "    self.blocks = [\n",
        "        TransformerEncoderBlock(num_heads=self.num_heads,\n",
        "                                output_size=self.n_embed,\n",
        "                                num_tokens=self.num_tokens) for _ in range(self.num_layers)\n",
        "    ]\n",
        "    self.ln = nn.LayerNorm()\n",
        "    self.lang_model_head = nn.Dense(features=self.C)\n",
        "\n",
        "  def __call__(self, block_of_tokens: jnp.array, training: bool):\n",
        "    \"\"\"Accepts a block of tokens, like [0, 1, 2, 3, 4, 5, 6, 7].\"\"\"\n",
        "    # generate emb for each token. output: (num_tokens, n_embed)\n",
        "    token_embs = self.token_embedding_table(block_of_tokens)\n",
        "\n",
        "    # generate position embs for each token.\n",
        "    num_pos = block_of_tokens.shape[0]\n",
        "    positions = jnp.arange(0, num_pos)\n",
        "    pos_embs = self.pos_embedding_table(positions)\n",
        "\n",
        "    # generate actual input to attention, x, which is sum of token_embs + pos_embs\n",
        "    x = token_embs + pos_embs\n",
        "\n",
        "    # feed x into self-attention head.\n",
        "    # language model, forward pass, block_of_tokens\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.blocks[i](x, training)\n",
        "\n",
        "    x = self.ln(x)\n",
        "\n",
        "    # generate logits for each token. output: (T, channels for info -- C)\n",
        "    token_logits = self.lang_model_head(x)\n",
        "\n",
        "    return token_logits"
      ],
      "metadata": {
        "id": "T-EplD3kJspx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import chex\n",
        "from chex._src import fake\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "from dataclasses import dataclass\n",
        "import importlib\n",
        "import pdb\n",
        "\n",
        "# import dataset\n",
        "# import model\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "    key: jax.random.KeyArray\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    BATCH_SIZE: int = 256\n",
        "    BLOCK_SIZE: int = 64\n",
        "    T: int = 64\n",
        "    n_embed: int = 256\n",
        "    num_heads: int = 8\n",
        "    num_layers: int = 6\n",
        "\n",
        "config = Config()\n",
        "\n",
        "random_key = jax.random.PRNGKey(99)\n",
        "\n",
        "# Initialize model\n",
        "lm_model = LanguageModel(vocab_size=65,\n",
        "                      n_embed=config.n_embed,\n",
        "                      num_tokens=config.BLOCK_SIZE,\n",
        "                      num_heads=config.num_heads,\n",
        "                      num_layers=config.num_layers)\n",
        "sample_block_of_tokens = jnp.ones(shape=(config.T,), dtype=jnp.int32)\n",
        "output, params = lm_model.init_with_output(jax.random.PRNGKey(99), sample_block_of_tokens, training=False)\n",
        "params = params[\"params\"]\n",
        "\n",
        "def model_apply(params, inputs, training):\n",
        "    return lm_model.apply({\"params\": params}, inputs, training)\n",
        "\n",
        "# Vectorize model apply function\n",
        "model_apply_batch = jax.vmap(model_apply, in_axes=(None, 0, None), out_axes=(0))\n",
        "\n",
        "PER_HOST_BATCH_SIZE = config.BATCH_SIZE // jax.device_count()\n",
        "\n",
        "# Define forward pass\n",
        "def forward_pass(params, state, batch, dropout_key):\n",
        "    inputs, targets = batch\n",
        "    logits = state.apply_fn(params, inputs, True, dropout_key)\n",
        "\n",
        "    chex.assert_shape(inputs, (PER_HOST_BATCH_SIZE, config.BLOCK_SIZE))\n",
        "    chex.assert_shape(targets, (PER_HOST_BATCH_SIZE, config.BLOCK_SIZE))\n",
        "\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
        "    loss = loss.mean()\n",
        "    return loss\n",
        "\n",
        "# Define training step\n",
        "def train_step(state, inputs, targets, dropout_key):\n",
        "    dropout_key = jax.random.fold_in(key=dropout_key, data=state.step)\n",
        "\n",
        "    batch = inputs, targets\n",
        "\n",
        "    grad_fn = jax.value_and_grad(forward_pass, argnums=(0))\n",
        "    loss, grads = grad_fn(state.params, state, batch, dropout_key)\n",
        "\n",
        "    loss = jax.lax.pmean(loss, axis_name=\"devices\")\n",
        "    grads = jax.lax.pmean(grads, axis_name=\"devices\")\n",
        "\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss\n",
        "\n",
        "# Initialize optimizer and training state\n",
        "opt = optax.adam(learning_rate=0.0001)\n",
        "state = TrainState.create(apply_fn=model_apply_batch, params=params, tx=opt, key=random_key)\n",
        "\n",
        "# pmap the train_step.\n",
        "train_step_pmap = jax.jit(jax.pmap(train_step, in_axes=(0, 0, 0, None), out_axes=(0), axis_name=\"devices\"))\n",
        "states = jax.device_put_replicated(state, jax.local_devices())"
      ],
      "metadata": {
        "id": "88FqbWNaN5H6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fake_pmap = chex.fake_pmap_and_jit(enable_jit_patching=fake_jit, enable_pmap_patching=fake_pmap)\n",
        "# fake_pmap.start()\n",
        "num_epochs = 1 # 20\n",
        "steps_per_epoch = 1 # len(data.train_data) // config.BATCH_SIZE\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"epoch: \", epoch)\n",
        "\n",
        "  for step in range(steps_per_epoch):\n",
        "    random_key, random_subkey = jax.random.split(random_key)\n",
        "\n",
        "    inputs, targets = get_batch(train_dataset)\n",
        "\n",
        "    # create device dimension for minibatch\n",
        "    inputs = inputs.reshape((jax.device_count(), -1, inputs.shape[-1]))\n",
        "    targets = targets.reshape((jax.device_count(), -1, targets.shape[-1]))\n",
        "\n",
        "    states, loss = train_step_pmap(states, inputs, targets, random_subkey)\n",
        "    print(\"loss\", loss[0], \"epoch\", epoch) if epoch % 510 == 0 else None\n",
        "\n",
        "# fake_pmap.stop()"
      ],
      "metadata": {
        "id": "siy-deXXO_C3",
        "outputId": "e1393bf9-fa65-47f1-e867-5a7ac0dafa51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InconclusiveDimensionOperation",
          "evalue": "Cannot divide evenly the sizes of shapes (4, 8) and (8, -1, 8)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInconclusiveDimensionOperation\u001b[0m            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-40ef16902f58>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# create device dimension for minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_reshape\u001b[0;34m(a, order, *args)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m   \u001b[0mnewshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compute_newshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0morder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"C\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_compute_newshape\u001b[0;34m(a, newshape)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0miterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m   \u001b[0mnewshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewshape\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0miterable\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnewshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m   return tuple(- core.divide_shape_sizes(np.shape(a), newshape)\n\u001b[0m\u001b[1;32m    786\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_equal_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                for d in newshape)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0miterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m   \u001b[0mnewshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewshape\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0miterable\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnewshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m   return tuple(- core.divide_shape_sizes(np.shape(a), newshape)\n\u001b[0m\u001b[1;32m    786\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_equal_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                for d in newshape)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mdivide_shape_sizes\u001b[0;34m(s1, s2)\u001b[0m\n\u001b[1;32m   1842\u001b[0m   \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m   \u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dim_handler_and_canonical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide_shape_sizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msame_shape_sizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mShape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mShape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mdivide_shape_sizes\u001b[0;34m(self, s1, s2)\u001b[0m\n\u001b[1;32m   1738\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msz1\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msz2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1740\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mInconclusiveDimensionOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cannot divide evenly the sizes of shapes {tuple(s1)} and {tuple(s2)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1741\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msz1\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0msz2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInconclusiveDimensionOperation\u001b[0m: Cannot divide evenly the sizes of shapes (4, 8) and (8, -1, 8)"
          ]
        }
      ]
    }
  ]
}