{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Install dependencies for CPU</h1>"
      ],
      "metadata": {
        "id": "16WhNG98gurS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6427RRkAbysU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef730fd-a957-4eac-c685-b586f8ddc402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-08 03:39:42--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-02-08 03:39:42 (49.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q --upgrade pip # To support manylinux2010 wheels.\n",
        "# !pip install -q --upgrade jax jaxlib # CPU-only\n",
        "# !pip install -q --upgrade jaxtyping\n",
        "# !pip install -q --upgrade flax"
      ],
      "metadata": {
        "id": "5qQHP7SGLTlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Mapping, Tuple\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jrand\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state  # Useful dataclass to keep train state\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import pdb\n",
        "\n",
        "def println(*args):\n",
        "  for arg in args:\n",
        "    print(arg)\n"
      ],
      "metadata": {
        "id": "OZqEXlfmfuwo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset pipeline"
      ],
      "metadata": {
        "id": "TeccUW1zg-V-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create chars vocubulary using all the unique characters in the text.\n",
        "chars = sorted(list(set(text)))\n",
        "VOCAB_SIZE = len(chars)\n",
        "\n",
        "# Create mapping from characters to integers.\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "\n",
        "# Create reverse mapping from integers to characters.\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Create encode, decode function.\n",
        "def encode(s: str, stoi: Mapping[str, int]) -> List[int]:\n",
        "  return [stoi[c] for c in s]\n",
        "\n",
        "def decode(tokens: List[int], itos: Mapping[int, str]) -> str:\n",
        "  return ''.join([itos[i] for i in tokens])\n",
        "\n",
        "println(encode(\"hii there\", stoi), decode(encode(\"hii there\", stoi), itos))\n",
        "\n",
        "# Let's now split up the data into train and validation sets.\n",
        "data = jnp.array(encode(text, stoi), dtype=jnp.int64)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Below would result in a minibatch size of 32.\n",
        "BATCH_SIZE = 4 # how many independent sequences will we process in parallel?\n",
        "BLOCK_SIZE = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices(train_data)\n",
        "                .batch(BLOCK_SIZE+1)\n",
        "                .map(lambda input: (input[:BLOCK_SIZE], input[1:BLOCK_SIZE+1]),\n",
        "                     num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                .batch(BATCH_SIZE)\n",
        "                .repeat()\n",
        "                .as_numpy_iterator())\n",
        "val_dataset = (tf.data.Dataset.from_tensor_slices(val_data)\n",
        "                .batch(BLOCK_SIZE+1)\n",
        "                .map(lambda input: (input[:BLOCK_SIZE], input[1:BLOCK_SIZE+1]),\n",
        "                     num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                .batch(BATCH_SIZE)\n",
        "                .repeat()\n",
        "                .as_numpy_iterator())\n",
        "\n",
        "def get_batch(training: bool = True):\n",
        "  if not training:\n",
        "    val_batch = next(val_dataset)\n",
        "    return jnp.array(val_batch)\n",
        "\n",
        "  train_batch = next(train_dataset)\n",
        "  return jnp.array(train_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYycQ8ocg2sy",
        "outputId": "c5c4e311-024a-4a60-97c2-80a3618eca90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-560c18dc3230>:24: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  data = jnp.array(encode(text, stoi), dtype=jnp.int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test dataset pipeline"
      ],
      "metadata": {
        "id": "sTAGaeSwXeG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch()\n",
        "println(\"inputs\", xb, \"inputs shape\", xb.shape)\n",
        "println(\"targets\", yb, \"targets shape\", yb.shape)\n",
        "for b in range(BATCH_SIZE): # batch dimension\n",
        "    for t in range(BLOCK_SIZE): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YTqHDylg6cY",
        "outputId": "795456c7-cd78-4540-c872-109c44ccf2c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs\n",
            "[[18 47 56 57 58  1 15 47]\n",
            " [47 64 43 52 10  0 14 43]\n",
            " [53 56 43  1 61 43  1 54]\n",
            " [53 41 43 43 42  1 39 52]]\n",
            "inputs shape\n",
            "(4, 8)\n",
            "targets\n",
            "[[47 56 57 58  1 15 47 58]\n",
            " [64 43 52 10  0 14 43 44]\n",
            " [56 43  1 61 43  1 54 56]\n",
            " [41 43 43 42  1 39 52 63]]\n",
            "targets shape\n",
            "(4, 8)\n",
            "when input is [18] the target: 47\n",
            "when input is [18, 47] the target: 56\n",
            "when input is [18, 47, 56] the target: 57\n",
            "when input is [18, 47, 56, 57] the target: 58\n",
            "when input is [18, 47, 56, 57, 58] the target: 1\n",
            "when input is [18, 47, 56, 57, 58, 1] the target: 15\n",
            "when input is [18, 47, 56, 57, 58, 1, 15] the target: 47\n",
            "when input is [18, 47, 56, 57, 58, 1, 15, 47] the target: 58\n",
            "when input is [47] the target: 64\n",
            "when input is [47, 64] the target: 43\n",
            "when input is [47, 64, 43] the target: 52\n",
            "when input is [47, 64, 43, 52] the target: 10\n",
            "when input is [47, 64, 43, 52, 10] the target: 0\n",
            "when input is [47, 64, 43, 52, 10, 0] the target: 14\n",
            "when input is [47, 64, 43, 52, 10, 0, 14] the target: 43\n",
            "when input is [47, 64, 43, 52, 10, 0, 14, 43] the target: 44\n",
            "when input is [53] the target: 56\n",
            "when input is [53, 56] the target: 43\n",
            "when input is [53, 56, 43] the target: 1\n",
            "when input is [53, 56, 43, 1] the target: 61\n",
            "when input is [53, 56, 43, 1, 61] the target: 43\n",
            "when input is [53, 56, 43, 1, 61, 43] the target: 1\n",
            "when input is [53, 56, 43, 1, 61, 43, 1] the target: 54\n",
            "when input is [53, 56, 43, 1, 61, 43, 1, 54] the target: 56\n",
            "when input is [53] the target: 41\n",
            "when input is [53, 41] the target: 43\n",
            "when input is [53, 41, 43] the target: 43\n",
            "when input is [53, 41, 43, 43] the target: 42\n",
            "when input is [53, 41, 43, 43, 42] the target: 1\n",
            "when input is [53, 41, 43, 43, 42, 1] the target: 39\n",
            "when input is [53, 41, 43, 43, 42, 1, 39] the target: 52\n",
            "when input is [53, 41, 43, 43, 42, 1, 39, 52] the target: 63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Bigram Model"
      ],
      "metadata": {
        "id": "a3ozrVm0XiVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLangModel(nn.Module):\n",
        "  \"\"\"Reads one char and predicits the next char.\"\"\"\n",
        "  vocab_size: int\n",
        "\n",
        "  def setup(self):\n",
        "    super().setup()\n",
        "    self.token_embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.vocab_size)\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    # Run block size inputs through embedding lookup.\n",
        "    # For each char, you get the logit predicted for that char.\n",
        "    # Then, you use the target token for that input and do a cross_entropy_loss.\n",
        "    logits = self.token_embedding_table(inputs)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "2eyyiNsw_YL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I'll make the flax model accept\n",
        "`[block size worth tokens, token]`\n",
        "\n",
        "## I'll then use vmap to make the model accept batches of data.\n",
        "`[batch dim, block size worth of tokens, token]`"
      ],
      "metadata": {
        "id": "bLZdjG6cVxij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input_row = jnp.ones(shape=[1, 1], dtype=jnp.int32)\n",
        "sample_input_row"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkxBwwipK6Yc",
        "outputId": "89172d37-3e18-4d03-ed0d-4357ea815504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLangModel(vocab_size=65)\n",
        "output, params = model.init_with_output(jrand.PRNGKey(99), sample_input_row)\n",
        "params = params[\"params\"]"
      ],
      "metadata": {
        "id": "dtb3A3KFCgF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make the model accepts batch of data\n",
        "`[batch, block of tokens, token_ids]`\n",
        "\n",
        "## To make it accept a batch, you need to use vmap."
      ],
      "metadata": {
        "id": "n0GfZYSQRE93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_apply_batch = jax.vmap(model.apply, in_axes=(None, 0), out_axes=(0))"
      ],
      "metadata": {
        "id": "nu8eFbAURSOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_apply_batch will accept\n",
        "# [batch, block of tokens, token ids]"
      ],
      "metadata": {
        "id": "eHo-zsVlWxNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample forward pass, loss and backward pass."
      ],
      "metadata": {
        "id": "BPYIijJYDSV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = get_batch()\n",
        "inputs, targets = batch\n",
        "println(\"inputs\", inputs, inputs.shape, \"targets\", targets.shape)"
      ],
      "metadata": {
        "id": "x9UFVRpPDeZI",
        "outputId": "ec06e340-8af9-4076-87d7-06669001bb77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs\n",
            "[[ 1 44 59 56 58 46 43 56]\n",
            " [ 1 46 43 39 56  1 51 43]\n",
            " [57 54 43 39 49  8  0  0]\n",
            " [50 50 10  0 31 54 43 39]]\n",
            "(4, 8)\n",
            "targets\n",
            "(4, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model_apply_batch({\"params\": params}, inputs)"
      ],
      "metadata": {
        "id": "JNlkzt-IRr5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output should shape [4, 8, 65]\n",
        "# batch size = 4\n",
        "# block of tokens = 8\n",
        "# token_id to embedding = 65\n",
        "\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiE6Yl0QXFml",
        "outputId": "ba82ecc5-805a-4e64-81e2-f322441b7951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 8, 65)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To do backward pass, you first need to compute grads.\n",
        "# In JAX, you use jax.grad to do a function transformation on the forward\n",
        "# function to get the gradient of the original function.\n",
        "# The grad is calculate wrt to the first param in the function.\n",
        "def forward_pass(params, batch):\n",
        "  inputs, targets = batch\n",
        "  logits = model_apply_batch({\"params\": params}, inputs)\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
        "  loss = loss.mean()\n",
        "  return loss"
      ],
      "metadata": {
        "id": "09vAczHeGDis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test forward pass.\n",
        "batch = get_batch()\n",
        "forward_pass(params=params, batch=batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCh6wzoHYWDm",
        "outputId": "e90fece8-405b-44b6-d121-9bf3ea15d45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(4.188369, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grad_fn = jax.value_and_grad(forward_pass, argnums=(0))  # differentiate wrt 0th pos argument."
      ],
      "metadata": {
        "id": "lJnYF1OtGzPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test forward pass and grads.\n",
        "# Grads would be the gradients for params.\n",
        "loss, grads = grad_fn(params, batch)\n",
        "println(\"loss\", loss, \"grads\", grads)"
      ],
      "metadata": {
        "id": "o3XKCpCJHQip",
        "outputId": "7f8a3d93-0f26-407d-b7bc-daea0e07089d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss\n",
            "4.188369\n",
            "grads\n",
            "{'token_embedding_table': {'embedding': Array([[0.00103511, 0.00097044, 0.00093559, ..., 0.00089521, 0.001064  ,\n",
            "        0.00093856],\n",
            "       [0.00203619, 0.00209828, 0.00198735, ..., 0.00196691, 0.00215726,\n",
            "        0.00207174],\n",
            "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
            "        0.        ],\n",
            "       ...,\n",
            "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
            "        0.        ],\n",
            "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
            "        0.        ],\n",
            "       [0.00040068, 0.00039527, 0.00040171, ..., 0.00042136, 0.00055311,\n",
            "        0.00042526]], dtype=float32)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply grads to params to get new params.\n",
        "lr = 0.001\n",
        "println(\"params before:\", params)\n",
        "params = jax.tree_map(lambda p, g: p - lr * g, params, grads)\n",
        "println(\"params after:\", params)"
      ],
      "metadata": {
        "id": "qLHB0BpQHfI1",
        "outputId": "6e0d0d4c-d484-413e-e159-30970dc28fbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params before:\n",
            "{'token_embedding_table': {'embedding': Array([[ 0.0752212 ,  0.01071652, -0.02585994, ..., -0.06997449,\n",
            "         0.10274917, -0.0226865 ],\n",
            "       [ 0.09400459,  0.12404279,  0.06972364, ...,  0.0593865 ,\n",
            "         0.1517611 ,  0.11131445],\n",
            "       [-0.0302137 , -0.07326671, -0.2515272 , ...,  0.20769818,\n",
            "         0.01281604,  0.03134193],\n",
            "       ...,\n",
            "       [-0.1394756 , -0.00640967, -0.07666602, ..., -0.2944119 ,\n",
            "         0.1187517 , -0.08573762],\n",
            "       [ 0.05703759, -0.11280773,  0.2570641 , ..., -0.02059634,\n",
            "        -0.02818088,  0.13305528],\n",
            "       [-0.12428083, -0.13785616, -0.12170236, ..., -0.07394623,\n",
            "         0.19811267, -0.06473607]], dtype=float32)}}\n",
            "params after:\n",
            "{'token_embedding_table': {'embedding': Array([[ 0.07522016,  0.01071555, -0.02586087, ..., -0.06997538,\n",
            "         0.1027481 , -0.02268744],\n",
            "       [ 0.09400256,  0.12404069,  0.06972165, ...,  0.05938453,\n",
            "         0.15175894,  0.11131237],\n",
            "       [-0.0302137 , -0.07326671, -0.2515272 , ...,  0.20769818,\n",
            "         0.01281604,  0.03134193],\n",
            "       ...,\n",
            "       [-0.1394756 , -0.00640967, -0.07666602, ..., -0.2944119 ,\n",
            "         0.1187517 , -0.08573762],\n",
            "       [ 0.05703759, -0.11280773,  0.2570641 , ..., -0.02059634,\n",
            "        -0.02818088,  0.13305528],\n",
            "       [-0.12428124, -0.13785656, -0.12170276, ..., -0.07394665,\n",
            "         0.19811212, -0.06473649]], dtype=float32)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing train step in flax\n",
        "## copy-pasting everything at one place and running a train step."
      ],
      "metadata": {
        "id": "LKPwDOFmIaSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLangModel(nn.Module):\n",
        "  \"\"\"Reads one char and predicits the next char.\"\"\"\n",
        "  vocab_size: int\n",
        "\n",
        "  def setup(self):\n",
        "    super().setup()\n",
        "    self.token_embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.vocab_size)\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    # Run block size inputs through embedding lookup.\n",
        "    # For each char, you get the logit predicted for that char.\n",
        "    # Then, you use the target token for that input and do a cross_entropy_loss.\n",
        "    logits = self.token_embedding_table(inputs)\n",
        "    return logits\n",
        "\n",
        "model = BigramLangModel(vocab_size=65)\n",
        "\n",
        "sample_input_row = jnp.ones(shape=[1, 1], dtype=jnp.int32)\n",
        "output, params = model.init_with_output(jrand.PRNGKey(99), sample_input_row)\n",
        "params = params[\"params\"]\n",
        "\n",
        "model_apply_batch = jax.vmap(model.apply, in_axes=(None, 0), out_axes=(0))\n",
        "\n",
        "def forward_pass(params, state, batch):\n",
        "  inputs, targets = batch\n",
        "  logits = state.apply_fn({\"params\": params}, inputs)\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
        "  loss = loss.mean()\n",
        "  return loss\n",
        "\n",
        "grad_fn = jax.value_and_grad(forward_pass, argnums=(0))  # differentiate wrt 0th pos argument.\n",
        "\n",
        "opt = optax.adam(learning_rate=0.001)\n",
        "state = train_state.TrainState.create(apply_fn=model_apply_batch, params=params, tx=opt)\n",
        "\n",
        "for epoch in range(1000):\n",
        "  batch = get_batch()\n",
        "  loss, grads = grad_fn(state.params, state, batch)\n",
        "  print(loss) if epoch%100==0 else None\n",
        "  state = state.apply_gradients(grads=grads)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtzCS8NzMJx9",
        "outputId": "34583d0f-e5b4-46f7-e7c6-26cff715b9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.1769814\n",
            "4.093553\n",
            "4.0517178\n",
            "3.9815784\n",
            "3.955411\n",
            "3.8293285\n",
            "3.7160275\n",
            "3.7963347\n",
            "3.7700095\n",
            "3.5433385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement code for generating tokens"
      ],
      "metadata": {
        "id": "Bze9wctCbxls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_token = \"n\"\n",
        "encode(input_token, stoi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2BUBirGMWo6",
        "outputId": "a04e3937-2140-4af6-af42-43a4a4a5b0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[52]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_token = jnp.array([[52]], dtype=jnp.int32)\n",
        "input_token.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p4A99r1NKx-",
        "outputId": "5bd0b908-63ae-4092-ed74-de9fd982eba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_token_logit = state.apply_fn({\"params\": state.params}, input_token)"
      ],
      "metadata": {
        "id": "cJwJPnIlNSEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_token_logit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df1oFKh9N4oj",
        "outputId": "c62e6dee-1f64-4689-d010-92b14aecff90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[-0.03682718,  0.3873823 , -0.6297037 , -0.49402413,\n",
              "         -0.5302313 , -0.06883954,  0.02130109, -0.38699383,\n",
              "          0.05293135, -0.67822653,  0.04108687, -0.17033677,\n",
              "         -0.21989107, -0.664077  , -0.5178356 , -0.8944765 ,\n",
              "         -0.47879392, -0.6437434 , -0.8325122 , -0.8247406 ,\n",
              "         -0.7639986 , -0.6349938 , -0.58913904, -0.71123016,\n",
              "         -0.6316706 , -0.8322299 , -0.61255544, -0.8065904 ,\n",
              "         -0.6916736 , -0.7578409 , -0.77268964, -0.66586775,\n",
              "         -0.48679668, -0.6974697 , -0.5410919 , -0.86016047,\n",
              "         -0.7141595 , -0.87438166, -0.67868704,  0.03718114,\n",
              "         -0.9563432 , -0.07171286,  0.3103663 ,  0.27640587,\n",
              "         -0.15164396,  0.25430372, -0.8608676 , -0.11117504,\n",
              "         -0.5252868 , -0.07216536,  0.02316949, -0.9069602 ,\n",
              "         -0.26114473,  0.19493026, -0.9803428 , -0.70663923,\n",
              "         -0.25509965,  0.25409326,  0.1060916 , -0.11378517,\n",
              "         -0.35723025, -0.7799293 , -0.53064156, -0.42475328,\n",
              "         -0.66108644]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_key = jrand.PRNGKey(99)\n",
        "key, split_key = jrand.split(init_key)"
      ],
      "metadata": {
        "id": "OiIl31K0cQVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_to_next_token = jrand.categorical(split_key, next_token_logit)\n",
        "next_to_next_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMY9sAsCczWf",
        "outputId": "6db935a6-f8b2-4f9c-97a6-07ca1602b5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[25]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode(next_to_next_token.tolist()[0], itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7gpbNLGCc8k-",
        "outputId": "29f2d01b-abd5-447b-bb32-9ab72336fd1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_to_next_to_next_logit = state.apply_fn({\"params\": state.params}, next_to_next_token)"
      ],
      "metadata": {
        "id": "9yoh9KIXdby-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key, split_key = jrand.split(key)"
      ],
      "metadata": {
        "id": "eT4SCAXidmBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_to_next_to_next_token = jrand.categorical(split_key, next_token_logit)\n",
        "\n",
        "next_to_next_to_next_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE6B49q2dkNm",
        "outputId": "d305c669-7354-4688-b536-877fcb4fd57d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[59]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode(next_to_next_to_next_token.tolist()[0], itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qGbhB8K7d2B0",
        "outputId": "ca5c990e-4351-4586-86c3-4a545bde6a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting together the generate code\n",
        "\n",
        "input_token = jnp.array([[52]], dtype=jnp.int32)\n",
        "key = jrand.PRNGKey(99)\n",
        "\n",
        "result = \"\"\n",
        "for i in range(100):\n",
        "  key, split_key = jrand.split(key)\n",
        "  next_token_logit = state.apply_fn({\"params\": state.params}, input_token)\n",
        "  next_token = jrand.categorical(split_key, next_token_logit)\n",
        "  next_token_decode = decode(next_token.tolist()[0], itos)\n",
        "  result = result + next_token_decode\n",
        "\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keaylzQod6rh",
        "outputId": "b4b1ea7e-3062-4681-db54-4f7a6657d3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MuTqF\n",
            "d$oMJ\n",
            " CiSOzjIftBqertiG,3gdghx,,.V,d .zbfa'$fXoeyu'l!m:oaBBQRcrEttkQm3u-r.v3LgdMVfxsx-;ga!kcPW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical trick in attention"
      ],
      "metadata": {
        "id": "G78p73EKNOJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## doing bag of words\n",
        "basically, in B, T, C\n",
        "at t-th token in a row of batch, just sum all the values upto t."
      ],
      "metadata": {
        "id": "vWZlcN7aNQ6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate T, C and write code which works for T, C.\n",
        "# Then, vmap it for batch\n",
        "\n",
        "# Using tokens = 4\n",
        "# Using each token with channel = 2 to make it easy to visualize\n",
        "T, C = 4, 2\n",
        "\n",
        "key, split_key = jrand.split(jrand.PRNGKey(99))\n",
        "\n",
        "x = jrand.normal(split_key, (T, C))\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x3NkSoBNsTg",
        "outputId": "0b7a9884-57e5-4b76-f914-ce08694ed1c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[ 0.2628779 , -0.1837252 ],\n",
              "       [ 0.38331428, -0.16180514],\n",
              "       [ 1.4986674 ,  1.10728   ],\n",
              "       [ 1.1535788 ,  0.9676542 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### version 1: using for loop"
      ],
      "metadata": {
        "id": "ZFlsH67YZqQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bow_attention(x: jnp.array, T: int, C: int):\n",
        "  \"\"\"Operates on a single row within batchs\n",
        "\n",
        "    It calculates bow attention by summing all\n",
        "    token channels prev + current token channels.\n",
        "  \"\"\"\n",
        "  xbow = jnp.zeros(shape=(T, C))\n",
        "  for token in range(T):\n",
        "    xprev = x[:token]\n",
        "    xcurrent = x[token:token+1]\n",
        "\n",
        "    current_bow = jnp.mean(jnp.concatenate([xprev, xcurrent], axis=0), axis=0)\n",
        "    xbow = xbow.at[token].set(current_bow)\n",
        "  return xbow"
      ],
      "metadata": {
        "id": "jtZjg_WyS3pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_attention(x, T, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwDgkObLJh5X",
        "outputId": "b37e04e0-2152-4ccd-87a4-bbe165037341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[ 0.2628779 , -0.1837252 ],\n",
              "       [ 0.3230961 , -0.17276517],\n",
              "       [ 0.7149532 ,  0.25391656],\n",
              "       [ 0.8246096 ,  0.432351  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test bow_attention using non-random tensor.\n",
        "test_numbers = jnp.arange(1, 5).reshape(-1, 1)\n",
        "test_arr = jnp.tile(test_numbers, (1, C))\n",
        "bow_attention(test_arr, T, C)"
      ],
      "metadata": {
        "id": "aDTyU_cJCQaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba388bdf-f828-4e2d-e7d3-1f386f3977b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[1. , 1. ],\n",
              "       [1.5, 1.5],\n",
              "       [2. , 2. ],\n",
              "       [2.5, 2.5]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### version 2: Starting to write it using matmul"
      ],
      "metadata": {
        "id": "iCEfpKc4Juqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDJaLAS9JyAy",
        "outputId": "258b2bf2-b7d8-4e67-8732-e0be07259673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[ 0.2628779 , -0.1837252 ],\n",
              "       [ 0.38331428, -0.16180514],\n",
              "       [ 1.4986674 ,  1.10728   ],\n",
              "       [ 1.1535788 ,  0.9676542 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = jnp.array([[1.0, 0., 0., 0.],\n",
        " [0.5, 0.5, 0., 0.],\n",
        " [0.333, 0.333, 0.333, 0.],\n",
        " [0.25, 0.25, 0.25, 0.25]])"
      ],
      "metadata": {
        "id": "KseF9l2cJ550"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tril = jnp.tril(jnp.ones(shape=(T, T)))\n",
        "tril"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HBRJlxnQ9Wu",
        "outputId": "229d6295-e302-43d2-aad8-e288f1cc13ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.sum(tril, axis=1, keepdims=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbHxqFspWcN0",
        "outputId": "5a8e2f05-0469-43c5-a6b8-4aae471b8b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[1.],\n",
              "       [2.],\n",
              "       [3.],\n",
              "       [4.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wei(T: int):\n",
        "  tril = jnp.tril(jnp.ones(shape=(T, T)))\n",
        "  return tril/jnp.sum(tril, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "naS4RF-FWfEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_wei(T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-cr82EcWq5P",
        "outputId": "5001e475-14ab-4b6e-b460-b96f79f36ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.5       , 0.5       , 0.        , 0.        ],\n",
              "       [0.33333334, 0.33333334, 0.33333334, 0.        ],\n",
              "       [0.25      , 0.25      , 0.25      , 0.25      ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting together the bow attention calculation using matmul\n",
        "def bow_attention_matmul(x: jnp.array, T: int, C: int):\n",
        "  tril = jnp.tril(jnp.ones(shape=(T, T)))\n",
        "  wei = tril/jnp.sum(tril, axis=1, keepdims=True)\n",
        "\n",
        "  return jnp.dot(wei, x)\n"
      ],
      "metadata": {
        "id": "D2za5ZmdE9fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_attention_matmul(x, T, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P96Sm6qXHfi",
        "outputId": "b2145fb2-854d-4c12-9f15-da4e99700a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[ 0.2628779 , -0.1837252 ],\n",
              "       [ 0.3230961 , -0.17276517],\n",
              "       [ 0.7149532 ,  0.25391656],\n",
              "       [ 0.8246096 ,  0.432351  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### version 3: use softmax to generate wei matrix\n",
        "so that it can be learnable?"
      ],
      "metadata": {
        "id": "odn4VPr3cdaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tril = jnp.tril(jnp.ones(shape=(T, T)))\n",
        "tril"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFNaw1Ufcmk-",
        "outputId": "05e668c1-4182-4e77-fc7e-ba7be0a876c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you start wei as all zeros\n",
        "wei = jnp.zeros(shape=(T, T))\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5e1e7Bn0WhD",
        "outputId": "61a500e8-10d2-44d2-8b59-3b313aa6ff43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# but now, we modify wei such that whenever tril==0, we put -inf into wei\n",
        "wei = jnp.where(tril==0, -jnp.inf, wei)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgL4FNfI0cFk",
        "outputId": "506b5af9-d596-4459-df4a-885735cfab85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# next we take softmax along row, that is dim==-1\n",
        "wei = nn.softmax(wei, axis=-1)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMzvM4Q50loZ",
        "outputId": "16210e84-47e0-41ca-f399-aa5235c41399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.5       , 0.5       , 0.        , 0.        ],\n",
              "       [0.33333334, 0.33333334, 0.33333334, 0.        ],\n",
              "       [0.25      , 0.25      , 0.25      , 0.25      ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_attention(x: jnp.array, T:int, C:int):\n",
        "  \"\"\"Calculates attention for a row of tokens.\"\"\"\n",
        "  tril = jnp.tril(jnp.ones(shape=(T, T)))\n",
        "  wei = jnp.zeros(shape=(T, T))\n",
        "  wei = jnp.where(tril==0, -jnp.inf, wei)\n",
        "  wei = nn.softmax(wei, axis=-1)\n",
        "\n",
        "  return jnp.dot(wei, x)"
      ],
      "metadata": {
        "id": "NtMC5WcrdE7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_attention(x, T, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5XZ1TjsdpSH",
        "outputId": "96b9e0b8-c261-47d8-b277-e20da92cbe57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[ 0.2628779 , -0.1837252 ],\n",
              "       [ 0.3230961 , -0.17276517],\n",
              "       [ 0.7149532 ,  0.25391656],\n",
              "       [ 0.8246096 ,  0.432351  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_attention(test_arr, T, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpe729Ygd2V6",
        "outputId": "cb6638a5-2275-452e-94d2-71458d851238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[1. , 1. ],\n",
              "       [1.5, 1.5],\n",
              "       [2. , 2. ],\n",
              "       [2.5, 2.5]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_attention_batch = jax.vmap(calc_attention, in_axes=(0, None, None), out_axes=(0))"
      ],
      "metadata": {
        "id": "BQ_Sy-FEeHpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T, C = 8, 65\n",
        "test_numbers = jnp.arange(1, T+1).reshape(-1, 1)\n",
        "test_arr = jnp.tile(test_numbers, (1, C))\n",
        "\n",
        "# add batch dimension to test_arr\n",
        "test_arr_batch = test_arr[None, :]"
      ],
      "metadata": {
        "id": "ZpQKVgZwebqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test calc_attention_batch using get_batch\n",
        "calc_attention_batch(test_arr_batch, T, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsnmMhRmePJR",
        "outputId": "2e37b3f4-221b-42a6-f2fc-e6696fd2cf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[1.       , 1.       , 1.       , 1.       , 1.       ,\n",
              "         1.       , 1.       , 1.       , 1.       , 1.       ,\n",
              "         1.       , 1.       , 1.       , 1.       , 1.       ,\n",
              "         1.       , 1.       , 1.       , 1.       , 1.       ,\n",
              "         1.       , 1.       , 1.       , 1.       , 1.       ,\n",
              "         1.       , 1.       , 1.       , 1.       , 1.       ,\n",
              "         1.       , 1.       , 1.       , 1.       , 1.       ,\n",
              "         1.       , 1.       , 1.       , 1.       , 1.       ,\n",
              "         1.       , 1.       , 1.       , 1.       , 1.       ,\n",
              "         1.       , 1.       , 1.       , 1.       , 1.       ,\n",
              "         1.       , 1.       , 1.       , 1.       , 1.       ,\n",
              "         1.       , 1.       , 1.       , 1.       , 1.       ,\n",
              "         1.       , 1.       , 1.       , 1.       , 1.       ],\n",
              "        [1.5      , 1.5      , 1.5      , 1.5      , 1.5      ,\n",
              "         1.5      , 1.5      , 1.5      , 1.5      , 1.5      ,\n",
              "         1.5      , 1.5      , 1.5      , 1.5      , 1.5      ,\n",
              "         1.5      , 1.5      , 1.5      , 1.5      , 1.5      ,\n",
              "         1.5      , 1.5      , 1.5      , 1.5      , 1.5      ,\n",
              "         1.5      , 1.5      , 1.5      , 1.5      , 1.5      ,\n",
              "         1.5      , 1.5      , 1.5      , 1.5      , 1.5      ,\n",
              "         1.5      , 1.5      , 1.5      , 1.5      , 1.5      ,\n",
              "         1.5      , 1.5      , 1.5      , 1.5      , 1.5      ,\n",
              "         1.5      , 1.5      , 1.5      , 1.5      , 1.5      ,\n",
              "         1.5      , 1.5      , 1.5      , 1.5      , 1.5      ,\n",
              "         1.5      , 1.5      , 1.5      , 1.5      , 1.5      ,\n",
              "         1.5      , 1.5      , 1.5      , 1.5      , 1.5      ],\n",
              "        [2.       , 2.       , 2.       , 2.       , 2.       ,\n",
              "         2.       , 2.       , 2.       , 2.       , 2.       ,\n",
              "         2.       , 2.       , 2.       , 2.       , 2.       ,\n",
              "         2.       , 2.       , 2.       , 2.       , 2.       ,\n",
              "         2.       , 2.       , 2.       , 2.       , 2.       ,\n",
              "         2.       , 2.       , 2.       , 2.       , 2.       ,\n",
              "         2.       , 2.       , 2.       , 2.       , 2.       ,\n",
              "         2.       , 2.       , 2.       , 2.       , 2.       ,\n",
              "         2.       , 2.       , 2.       , 2.       , 2.       ,\n",
              "         2.       , 2.       , 2.       , 2.       , 2.       ,\n",
              "         2.       , 2.       , 2.       , 2.       , 2.       ,\n",
              "         2.       , 2.       , 2.       , 2.       , 2.       ,\n",
              "         2.       , 2.       , 2.       , 2.       , 2.       ],\n",
              "        [2.5      , 2.5      , 2.5      , 2.5      , 2.5      ,\n",
              "         2.5      , 2.5      , 2.5      , 2.5      , 2.5      ,\n",
              "         2.5      , 2.5      , 2.5      , 2.5      , 2.5      ,\n",
              "         2.5      , 2.5      , 2.5      , 2.5      , 2.5      ,\n",
              "         2.5      , 2.5      , 2.5      , 2.5      , 2.5      ,\n",
              "         2.5      , 2.5      , 2.5      , 2.5      , 2.5      ,\n",
              "         2.5      , 2.5      , 2.5      , 2.5      , 2.5      ,\n",
              "         2.5      , 2.5      , 2.5      , 2.5      , 2.5      ,\n",
              "         2.5      , 2.5      , 2.5      , 2.5      , 2.5      ,\n",
              "         2.5      , 2.5      , 2.5      , 2.5      , 2.5      ,\n",
              "         2.5      , 2.5      , 2.5      , 2.5      , 2.5      ,\n",
              "         2.5      , 2.5      , 2.5      , 2.5      , 2.5      ,\n",
              "         2.5      , 2.5      , 2.5      , 2.5      , 2.5      ],\n",
              "        [3.       , 3.       , 3.       , 3.       , 3.       ,\n",
              "         3.       , 3.       , 3.       , 3.       , 3.       ,\n",
              "         3.       , 3.       , 3.       , 3.       , 3.       ,\n",
              "         3.       , 3.       , 3.       , 3.       , 3.       ,\n",
              "         3.       , 3.       , 3.       , 3.       , 3.       ,\n",
              "         3.       , 3.       , 3.       , 3.       , 3.       ,\n",
              "         3.       , 3.       , 3.       , 3.       , 3.       ,\n",
              "         3.       , 3.       , 3.       , 3.       , 3.       ,\n",
              "         3.       , 3.       , 3.       , 3.       , 3.       ,\n",
              "         3.       , 3.       , 3.       , 3.       , 3.       ,\n",
              "         3.       , 3.       , 3.       , 3.       , 3.       ,\n",
              "         3.       , 3.       , 3.       , 3.       , 3.       ,\n",
              "         3.       , 3.       , 3.       , 3.       , 3.       ],\n",
              "        [3.5      , 3.5      , 3.5      , 3.5      , 3.5      ,\n",
              "         3.5      , 3.5      , 3.5      , 3.5      , 3.5      ,\n",
              "         3.5      , 3.5      , 3.5      , 3.5      , 3.5      ,\n",
              "         3.5      , 3.5      , 3.5      , 3.5      , 3.5      ,\n",
              "         3.5      , 3.5      , 3.5      , 3.5      , 3.5      ,\n",
              "         3.5      , 3.5      , 3.5      , 3.5      , 3.5      ,\n",
              "         3.5      , 3.5      , 3.5      , 3.5      , 3.5      ,\n",
              "         3.5      , 3.5      , 3.5      , 3.5      , 3.5      ,\n",
              "         3.5      , 3.5      , 3.5      , 3.5      , 3.5      ,\n",
              "         3.5      , 3.5      , 3.5      , 3.5      , 3.5      ,\n",
              "         3.5      , 3.5      , 3.5      , 3.5      , 3.5      ,\n",
              "         3.5      , 3.5      , 3.5      , 3.5      , 3.5      ,\n",
              "         3.5      , 3.5      , 3.5      , 3.5      , 3.5      ],\n",
              "        [4.0000005, 4.0000005, 4.0000005, 4.0000005, 4.0000005,\n",
              "         4.0000005, 4.0000005, 4.0000005, 4.0000005, 4.0000005,\n",
              "         4.0000005, 4.0000005, 4.0000005, 4.0000005, 4.0000005,\n",
              "         4.0000005, 4.0000005, 4.0000005, 4.0000005, 4.0000005,\n",
              "         4.0000005, 4.0000005, 4.0000005, 4.0000005, 4.0000005,\n",
              "         4.0000005, 4.0000005, 4.0000005, 4.0000005, 4.0000005,\n",
              "         4.0000005, 4.0000005, 4.0000005, 4.0000005, 4.0000005,\n",
              "         4.0000005, 4.0000005, 4.0000005, 4.0000005, 4.0000005,\n",
              "         4.0000005, 4.0000005, 4.0000005, 4.0000005, 4.0000005,\n",
              "         4.0000005, 4.0000005, 4.0000005, 4.0000005, 4.0000005,\n",
              "         4.0000005, 4.0000005, 4.0000005, 4.0000005, 4.0000005,\n",
              "         4.0000005, 4.0000005, 4.0000005, 4.0000005, 4.0000005,\n",
              "         4.0000005, 4.0000005, 4.0000005, 4.0000005, 4.0000005],\n",
              "        [4.5      , 4.5      , 4.5      , 4.5      , 4.5      ,\n",
              "         4.5      , 4.5      , 4.5      , 4.5      , 4.5      ,\n",
              "         4.5      , 4.5      , 4.5      , 4.5      , 4.5      ,\n",
              "         4.5      , 4.5      , 4.5      , 4.5      , 4.5      ,\n",
              "         4.5      , 4.5      , 4.5      , 4.5      , 4.5      ,\n",
              "         4.5      , 4.5      , 4.5      , 4.5      , 4.5      ,\n",
              "         4.5      , 4.5      , 4.5      , 4.5      , 4.5      ,\n",
              "         4.5      , 4.5      , 4.5      , 4.5      , 4.5      ,\n",
              "         4.5      , 4.5      , 4.5      , 4.5      , 4.5      ,\n",
              "         4.5      , 4.5      , 4.5      , 4.5      , 4.5      ,\n",
              "         4.5      , 4.5      , 4.5      , 4.5      , 4.5      ,\n",
              "         4.5      , 4.5      , 4.5      , 4.5      , 4.5      ,\n",
              "         4.5      , 4.5      , 4.5      , 4.5      , 4.5      ]]],      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting together new Bigram model"
      ],
      "metadata": {
        "id": "jegiwSi3fsMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLangModel(nn.Module):\n",
        "  \"\"\"Reads one char and predicits the next char.\"\"\"\n",
        "  vocab_size: int # number of vocabulary (number of rows of embedding table)\n",
        "  n_embed: int # embedding dim after lookup\n",
        "\n",
        "  def setup(self):\n",
        "    super().setup()\n",
        "    # number of channels you want to use for store info for each token.\n",
        "    self.C = self.vocab_size\n",
        "\n",
        "    self.token_embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.n_embed)\n",
        "\n",
        "    self.lang_model_head = nn.Dense(features=self.C)\n",
        "\n",
        "  def __call__(self, block_of_tokens: jnp.array):\n",
        "    \"\"\"Accepts a block of tokens.\"\"\"\n",
        "\n",
        "    # generate em for each token. output: (T, n_embed)\n",
        "    token_embs = self.token_embedding_table(block_of_tokens)\n",
        "\n",
        "    # generate logits for each token. output: (T, channels for info -- C)\n",
        "    token_logits = self.lang_model_head(token_embs)"
      ],
      "metadata": {
        "id": "PieI67T3gHcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add positional embeddings to the above."
      ],
      "metadata": {
        "id": "xgPfy8ogiHyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_of_tokens_example = jnp.ones(shape=(1, 8))\n",
        "block_of_tokens_example, block_of_tokens_example.shape, block_of_tokens_example.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZYnHm6djluj",
        "outputId": "cabed313-7349-4813-d027-d9dd50745c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([[1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32), (1, 8), 8)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_pos = block_of_tokens_example.shape[1]\n",
        "num_pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "388ZajvbkPwp",
        "outputId": "ec5dcb94-1106-4738-8de2-61d006d9c2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.arange(0, num_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCYxlaI0kReC",
        "outputId": "29f9f5fc-4bd9-43dd-9a17-4c0e01888916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  \"\"\"Reads one char and predicits the next char.\"\"\"\n",
        "  vocab_size: int # number of vocabulary (number of rows of embedding table)\n",
        "  n_embed: int # embedding dim after lookup\n",
        "\n",
        "  block_size: int # T, i.e., number of tokens attention block is looking at once\n",
        "\n",
        "  def setup(self):\n",
        "    super().setup()\n",
        "    # number of channels you want to use for store info for each token.\n",
        "    self.C = self.vocab_size\n",
        "\n",
        "    self.token_embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.n_embed)\n",
        "\n",
        "    self.pos_embedding_table = nn.Embed(num_embeddings=self.block_size, features=self.n_embed)\n",
        "\n",
        "    self.lang_model_head = nn.Dense(features=self.C)\n",
        "\n",
        "  def __call__(self, block_of_tokens: jnp.array):\n",
        "    \"\"\"Accepts a block of tokens, like [0, 1, 2, 3, 4, 5, 6, 7].\"\"\"\n",
        "\n",
        "    # generate em for each token. output: (T, n_embed)\n",
        "    token_embs = self.token_embedding_table(block_of_tokens)\n",
        "\n",
        "    # generate position embs for each token.\n",
        "    ## get token positions.\n",
        "    num_pos = block_of_tokens.shape[0]\n",
        "    positions = jnp.arange(0, num_pos)\n",
        "    pos_embs = self.pos_embedding_table(positions)\n",
        "\n",
        "    # generate actual input to attention, x, which is sum of token_embs + pos_embs\n",
        "    x = token_embs + pos_embs\n",
        "\n",
        "    # generate logits for each token. output: (T, channels for info -- C)\n",
        "    token_logits = self.lang_model_head(x)\n"
      ],
      "metadata": {
        "id": "wd0Z83BuiLQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding single head attention."
      ],
      "metadata": {
        "id": "YwcW1HNy2BI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (copy-pasting from top)\n",
        "# Here, wei has uniform attention scores to previous tokens.\n",
        "# That is token in Tth position, is assuming that\n",
        "# each previous token has same amount of info.\n",
        "\n",
        "# But we want each Tth token to learn what to pay attention to.\n",
        "\n",
        "# So, we have each token emit \"keys\" -- info I have\n",
        "# Each token will emit \"query\" -- what I'm looking for\n",
        "# wei becomes the dot prodct of \"keys\" and \"query\" -- higher the dot product higher the match between\n",
        "# what I'm look for and what some previous token has.\n",
        "def calc_attention(x: jnp.array, T:int, C:int):\n",
        "  \"\"\"Calculates attention for a row of tokens.\"\"\"\n",
        "  tril = jnp.tril(jnp.ones(shape=(T, T)))\n",
        "  wei = jnp.zeros(shape=(T, T))\n",
        "  wei = jnp.where(tril==0, -jnp.inf, wei)\n",
        "  wei = nn.softmax(wei, axis=-1)\n",
        "\n",
        "  return jnp.dot(wei, x)"
      ],
      "metadata": {
        "id": "wC4gleJIy8Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_info_size = 16 # head_size, each token produces vector of this size for key, query\n",
        "\n",
        "# key, query will take vector of size C.\n",
        "# i.e., channels containing info of token and will output token_info_size\n",
        "key_layer = nn.Dense(token_info_size, use_bias=False)\n",
        "\n",
        "query_layer = nn.Dense(token_info_size, use_bias=False)"
      ],
      "metadata": {
        "id": "FHOVay-Y2Mss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (tokens, channel info for each)\n",
        "# (T, C)\n",
        "\n",
        "# for easy visualization, T=4, C=2\n",
        "T=4; C=2\n",
        "x = jrand.normal(jrand.PRNGKey(999), shape=(T, C))\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFKElCQe3kvE",
        "outputId": "650b7d1c-20ea-42eb-a2f1-73ef9df09fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[ 0.27297866, -0.6993713 ],\n",
              "       [ 0.428855  , -1.5621939 ],\n",
              "       [-0.05503325,  0.18392533],\n",
              "       [-0.18410844,  0.53945136]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prng = jrand.PRNGKey(9999)\n",
        "key, split_key = jrand.split(prng)"
      ],
      "metadata": {
        "id": "4uGkOUsJ4df3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keys emitted by each token.\n",
        "kparams = key_layer.init(split_key, x)[\"params\"]\n",
        "keys = key_layer.apply({\"params\": kparams}, x)\n",
        "\n",
        "# queries emitted by each token\n",
        "# NOTE: each token parallely and indpendently emits its \"keys\" and \"queries\"\n",
        "qparams = query_layer.init(split_key, x)[\"params\"]\n",
        "queries = query_layer.apply({\"params\": qparams}, x)\n",
        "\n",
        "keys.shape, queries.shape # each are (T, 16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ku4_M434A7P",
        "outputId": "1d9f1a83-387f-4480-de2f-2d2b56efada5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4, 16), (4, 16))"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOW, wei becomes this dot product between keys and querys\n",
        "wei = jnp.dot(queries, keys.T)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kafXGmJ948bX",
        "outputId": "0a90e5ea-c021-4f2c-d5fe-8314c259de78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[ 6.0750933, 12.761856 , -1.5228109, -4.5677843],\n",
              "       [12.761856 , 27.052605 , -3.221544 , -9.631147 ],\n",
              "       [-1.5228109, -3.221544 ,  0.3838081,  1.1482861],\n",
              "       [-4.5677843, -9.631147 ,  1.1482861,  3.4396741]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  tril = jnp.tril(jnp.ones(shape=(T, T)))\n",
        "\n",
        "  # Don't initialize wei as zeros\n",
        "  # wei = jnp.zeros(shape=(T, T))\n",
        "  wei = jnp.where(tril==0, -jnp.inf, wei)\n",
        "  wei = nn.softmax(wei, axis=-1)\n",
        "  wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCMiX_qF1K3L",
        "outputId": "fb47edbc-6fe1-46ce-a437-020474a2c5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
              "       [6.2173666e-07, 9.9999940e-01, 0.0000000e+00, 0.0000000e+00],\n",
              "       [1.2637094e-01, 2.3115156e-02, 8.5051382e-01, 0.0000000e+00],\n",
              "       [3.0229829e-04, 1.9118115e-06, 9.1810778e-02, 9.0788496e-01]],      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# each token also produces \"value\", which is what we would multiply with wei.\n",
        "# so, wei is attention score.\n",
        "# whenever the attention score is high, we want to take its value."
      ],
      "metadata": {
        "id": "VCpDFskA6PFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combing things from above and adding value layer as well."
      ],
      "metadata": {
        "id": "XNYPY8vr6pQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (tokens, channel info for each)\n",
        "# (T, C)\n",
        "\n",
        "# for easy visualization, T=4, C=2\n",
        "T=4; C=2\n",
        "x = jrand.normal(jrand.PRNGKey(999), shape=(T, C))\n",
        "\n",
        "key, split_key = jrand.split(jrand.PRNGKey(9999))\n",
        "\n",
        "token_info_size = 16 # head_size\n",
        "\n",
        "key_layer = nn.Dense(token_info_size, use_bias=False)\n",
        "query_layer = nn.Dense(token_info_size, use_bias=False)\n",
        "value_layer = nn.Dense(token_info_size, use_bias=False)\n",
        "\n",
        "\n",
        "keys = key_layer.apply(key_layer.init(split_key, x), x) # (T, token_info_size)\n",
        "queries = query_layer.apply(query_layer.init(split_key, x), x)\n",
        "values = value_layer.apply(value_layer.init(split_key, x), x) # (T, token_info_size)\n",
        "\n",
        "tril = jnp.tril(jnp.ones(shape=(T, T)))\n",
        "\n",
        "wei = jnp.dot(queries, keys.T) # (T, T)\n",
        "wei = jnp.where(tril==0, -jnp.inf, wei)\n",
        "wei = nn.softmax(wei, axis=-1)\n",
        "\n",
        "\n",
        "out = jnp.dot(wei, values) # (T, T) * (T, token_info_size)\n",
        "\n",
        "# shape should be (T, token_info_size)\n",
        "# i.e., (4, 16)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3j_56B_6tms",
        "outputId": "8caaf24a-2c7e-4742-e9d8-fb00b9cdacf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## self-attention vs cross-attention: https://youtu.be/kCc8FmEb1nY?t=4542"
      ],
      "metadata": {
        "id": "SdFA175m9cOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaled attention -- dividing wei*value by squared root of head_size https://youtu.be/kCc8FmEb1nY?t=4638"
      ],
      "metadata": {
        "id": "Xx8fa3aC-CLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wei = jnp.dot(queries, keys.T) * C**0.5 # (T, T)\n",
        "wei = jnp.where(tril==0, -jnp.inf, wei)\n",
        "wei = nn.softmax(wei, axis=-1)\n",
        "\n",
        "\n",
        "out = jnp.dot(wei, values) # (T, T) * (T, token_info_size)\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAsVpe_RFfkJ",
        "outputId": "873ca2df-4178-4c2b-981c-028171cc2667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[ 0.15091053,  1.1747676 , -0.27543876,  0.6276668 , -0.6659185 ,\n",
              "        -0.7521053 ,  0.15832554,  0.61729145,  0.5173192 , -1.0826657 ,\n",
              "         0.4595548 ,  0.38114175,  0.36432883, -0.12410479, -0.28500274,\n",
              "         0.8726827 ],\n",
              "       [ 0.15494807,  2.420155  , -0.75046575,  1.4276459 , -1.2494795 ,\n",
              "        -1.4867611 ,  0.18499118,  1.4277841 ,  1.0585895 , -2.3402715 ,\n",
              "         0.8965928 ,  0.6221145 ,  1.0077578 , -0.24458581, -0.6518733 ,\n",
              "         1.8538882 ],\n",
              "       [-0.0108901 , -0.18261386,  0.05756752, -0.1084154 ,  0.09365092,\n",
              "         0.11186212, -0.01323292, -0.10853641, -0.07983959,  0.1771509 ,\n",
              "        -0.06739505, -0.04610366, -0.07736284,  0.01839836,  0.04952013,\n",
              "        -0.14017412],\n",
              "       [-0.08724618, -0.85421604,  0.22667341, -0.47580495,  0.46656573,\n",
              "         0.5378475 , -0.09476726, -0.47136265, -0.37513095,  0.8030861 ,\n",
              "        -0.32692865, -0.2536266 , -0.30200323,  0.08864281,  0.21657023,\n",
              "        -0.642643  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement self-attention head.\n",
        "(copy-pasting from above code mostly into Flax module.)"
      ],
      "metadata": {
        "id": "8DrZZTi7zZm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  token_info_size: int # head_size; how much (emb dim) info each token emits for keys, queries, values.\n",
        "\n",
        "  T: int # block size; number of tokens in a block\n",
        "  C: int # channel info size: size of info channel of each token.\n",
        "\n",
        "\n",
        "  def setup(self):\n",
        "    super().setup()\n",
        "\n",
        "    # key, query will take vector of size C.\n",
        "    # i.e., channels containing info of token and will output token_info_size\n",
        "    self.key_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "    self.query_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "    self.value_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "\n",
        "\n",
        "  def __call__(self, block_of_tokens_with_info_channels: jnp.array):\n",
        "    \"\"\"Accepts a block of tokens with info channels, like (8, 65).\"\"\"\n",
        "\n",
        "    # TODO(ntnsonti): Double check; but tril should not be learnable according cGPT.\n",
        "    tril = jnp.tril(jnp.ones(shape=(self.T, self.T)))\n",
        "\n",
        "    keys = self.key_layer(block_of_tokens_with_info_channels) # (T, token_info_size)\n",
        "    queries = self.query_layer(block_of_tokens_with_info_channels)\n",
        "    values = self.value_layer(block_of_tokens_with_info_channels)\n",
        "\n",
        "    # compute attention score.\n",
        "    wei = jnp.dot(queries, keys.T) * C**0.5 # (T, T)\n",
        "    wei = jnp.where(tril==0, -jnp.inf, wei)\n",
        "    wei = nn.softmax(wei, axis=-1)\n",
        "\n",
        "\n",
        "    out = jnp.dot(wei, values) # (T, T) * (T, token_info_size))\n",
        "    return out # (T, token_info_size)\n"
      ],
      "metadata": {
        "id": "eYy0Xn9F-ipr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/kCc8FmEb1nY?t=4819"
      ],
      "metadata": {
        "id": "hER4RpvY5lnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  \"\"\"Reads one char and predicits the next char.\"\"\"\n",
        "  vocab_size: int # number of vocabulary (number of rows of embedding table)\n",
        "  n_embed: int # embedding dim after lookup\n",
        "\n",
        "  T: int # block size, i.e., number of tokens attention block is looking at once\n",
        "\n",
        "  def setup(self):\n",
        "    super().setup()\n",
        "    # number of channels you want to use for store info for each token.\n",
        "    self.C = self.vocab_size\n",
        "\n",
        "    self.token_embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.n_embed)\n",
        "\n",
        "    self.pos_embedding_table = nn.Embed(num_embeddings=self.T, features=self.n_embed)\n",
        "\n",
        "    self.self_attention_head = Head(token_info_size=self.n_embed, T=self.T, C=self.C)\n",
        "\n",
        "    self.lang_model_head = nn.Dense(features=self.C)\n",
        "\n",
        "  def __call__(self, block_of_tokens: jnp.array):\n",
        "    \"\"\"Accepts a block of tokens, like [0, 1, 2, 3, 4, 5, 6, 7].\"\"\"\n",
        "\n",
        "    # generate em for each token. output: (T, n_embed)\n",
        "    token_embs = self.token_embedding_table(block_of_tokens)\n",
        "\n",
        "    # generate position embs for each token.\n",
        "    ## get token positions.\n",
        "    num_pos = block_of_tokens.shape[0]\n",
        "    positions = jnp.arange(0, num_pos)\n",
        "    pos_embs = self.pos_embedding_table(positions)\n",
        "\n",
        "    # generate actual input to attention, x, which is sum of token_embs + pos_embs\n",
        "    x = token_embs + pos_embs\n",
        "\n",
        "    # feed x into self-attention head.\n",
        "    x = self.self_attention_head(x)\n",
        "\n",
        "    # generate logits for each token. output: (T, channels for info -- C)\n",
        "    token_logits = self.lang_model_head(x)\n",
        "\n",
        "    return token_logits\n"
      ],
      "metadata": {
        "id": "zV5KQ3OLGO_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traing the network."
      ],
      "metadata": {
        "id": "GBsD7xpOHMPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BLOCK_SIZE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgoUZysXICvH",
        "outputId": "14da1ff2-0019-4584-9b54-b00927553672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T = 8"
      ],
      "metadata": {
        "id": "12OMA67VIFlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=65, n_embed=32, T=BLOCK_SIZE)\n",
        "\n",
        "# Now, our language model needs to accept a block of tokens, not one-char at a time.\n",
        "# We'll then make it accept a batch of blocks of tokens using vmap.\n",
        "sample_block_of_tokens = jnp.ones(shape=(T), dtype=jnp.int32)\n",
        "output, params = model.init_with_output(jrand.PRNGKey(99), sample_block_of_tokens)\n",
        "params = params[\"params\"]\n",
        "\n",
        "model_apply_batch = jax.vmap(model.apply, in_axes=(None, 0), out_axes=(0))\n",
        "\n",
        "def forward_pass(params, state, batch):\n",
        "  inputs, targets = batch\n",
        "  logits = state.apply_fn({\"params\": params}, inputs)\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
        "  loss = loss.mean()\n",
        "  return loss\n",
        "\n",
        "grad_fn = jax.value_and_grad(forward_pass, argnums=(0))  # differentiate wrt 0th pos argument.\n",
        "\n",
        "opt = optax.adam(learning_rate=0.001)\n",
        "state = train_state.TrainState.create(apply_fn=model_apply_batch, params=params, tx=opt)\n",
        "\n",
        "for epoch in range(1000):\n",
        "  batch = get_batch()\n",
        "  loss, grads = grad_fn(state.params, state, batch)\n",
        "  print(loss) if epoch%100==0 else None\n",
        "  state = state.apply_gradients(grads=grads)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLPSyus-HWKi",
        "outputId": "efe58069-74ae-4ced-9e05-99fd18f04046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.204059\n",
            "3.9591136\n",
            "3.0337052\n",
            "2.8353043\n",
            "3.6666312\n",
            "2.9169798\n",
            "2.6496902\n",
            "2.8693814\n",
            "2.4801302\n",
            "2.4153466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head attention https://youtu.be/kCc8FmEb1nY?t=4925"
      ],
      "metadata": {
        "id": "BN7SglxvNUUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (copy-pasting single head attention from above)\n",
        "class Head(nn.Module):\n",
        "  token_info_size: int # head_size; how much (emb dim) info each token emits for keys, queries, values.\n",
        "\n",
        "  T: int # block size; number of tokens in a block\n",
        "  C: int # channel info size: size of info channel of each token.\n",
        "\n",
        "\n",
        "  def setup(self):\n",
        "    super().setup()\n",
        "\n",
        "    # key, query will take vector of size C.\n",
        "    # i.e., channels containing info of token and will output token_info_size\n",
        "    self.key_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "    self.query_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "    self.value_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "\n",
        "\n",
        "  def __call__(self, block_of_tokens_with_info_channels: jnp.array):\n",
        "    \"\"\"Accepts a block of tokens with info channels, like (8, 65).\"\"\"\n",
        "\n",
        "    # TODO(ntnsonti): Double check; but tril should not be learnable according cGPT.\n",
        "    tril = jnp.tril(jnp.ones(shape=(self.T, self.T)))\n",
        "\n",
        "    keys = self.key_layer(block_of_tokens_with_info_channels) # (T, token_info_size)\n",
        "    queries = self.query_layer(block_of_tokens_with_info_channels)\n",
        "    values = self.value_layer(block_of_tokens_with_info_channels)\n",
        "\n",
        "    # compute attention score.\n",
        "    wei = jnp.dot(queries, keys.T) * C**0.5 # (T, T)\n",
        "    wei = jnp.where(tril==0, -jnp.inf, wei)\n",
        "    wei = nn.softmax(wei, axis=-1)\n",
        "\n",
        "\n",
        "    out = jnp.dot(wei, values) # (T, T) * (T, token_info_size))\n",
        "    return out # (T, token_info_size)\n"
      ],
      "metadata": {
        "id": "cza12RTpN-ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You just run multiple attention heads in parallel and concatenate their output along channel dimension, i.e., dim==-1"
      ],
      "metadata": {
        "id": "KiT6OH_WOBNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  token_info_size: int\n",
        "\n",
        "  T: int\n",
        "  C: int\n",
        "\n",
        "  def setup(self):\n",
        "    super().setup()\n",
        "\n",
        "    self.heads = [Head(token_info_size=self.token_info_size, T=self.T, C=self.C) for _ in range(self.num_heads)]\n",
        "\n",
        "  def __call__(self, block_of_tokens_with_info_channels: jnp.array):\n",
        "    out_from_each_head = jnp.array([h(block_of_tokens_with_info_channels) for h in self.heads])\n",
        "    return jnp.concatenate(out_from_each_head, axis=-1)\n",
        "\n"
      ],
      "metadata": {
        "id": "FZCNtnFANWV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  \"\"\"Reads one char and predicits the next char.\"\"\"\n",
        "  vocab_size: int # number of vocabulary (number of rows of embedding table)\n",
        "  n_embed: int # embedding dim after lookup\n",
        "\n",
        "  T: int # block size, i.e., number of tokens attention block is looking at once\n",
        "\n",
        "  def setup(self):\n",
        "    super().setup()\n",
        "    # number of channels you want to use for store info for each token.\n",
        "    self.C = self.vocab_size\n",
        "\n",
        "    self.token_embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.n_embed)\n",
        "\n",
        "    self.pos_embedding_table = nn.Embed(num_embeddings=self.T, features=self.n_embed)\n",
        "\n",
        "    # *** new ***\n",
        "    # Since, there are 4 heads, each head only needs to output token_info of size 8.\n",
        "    # Concantenate token_info from all 4 heards, gives us 32\n",
        "    self.self_attention_heads = MultiHeadAttention(num_heads=4, token_info_size=int(self.n_embed/4), T=self.T, C=self.C)\n",
        "\n",
        "    self.lang_model_head = nn.Dense(features=self.C)\n",
        "\n",
        "  def __call__(self, block_of_tokens: jnp.array):\n",
        "    \"\"\"Accepts a block of tokens, like [0, 1, 2, 3, 4, 5, 6, 7].\"\"\"\n",
        "\n",
        "    # generate em for each token. output: (T, n_embed)\n",
        "    token_embs = self.token_embedding_table(block_of_tokens)\n",
        "\n",
        "    # generate position embs for each token.\n",
        "    ## get token positions.\n",
        "    num_pos = block_of_tokens.shape[0]\n",
        "    positions = jnp.arange(0, num_pos)\n",
        "    pos_embs = self.pos_embedding_table(positions)\n",
        "\n",
        "    # generate actual input to attention, x, which is sum of token_embs + pos_embs\n",
        "    x = token_embs + pos_embs\n",
        "\n",
        "    # feed x into self-attention head.\n",
        "    x = self.self_attention_heads(x)\n",
        "\n",
        "    # generate logits for each token. output: (T, channels for info -- C)\n",
        "    token_logits = self.lang_model_head(x)\n",
        "\n",
        "    return token_logits\n"
      ],
      "metadata": {
        "id": "8og78QLYPPNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=65, n_embed=32, T=BLOCK_SIZE)\n",
        "\n",
        "# Now, our language model needs to accept a block of tokens, not one-char at a time.\n",
        "# We'll then make it accept a batch of blocks of tokens using vmap.\n",
        "sample_block_of_tokens = jnp.ones(shape=(T), dtype=jnp.int32)\n",
        "output, params = model.init_with_output(jrand.PRNGKey(99), sample_block_of_tokens)\n",
        "params = params[\"params\"]\n",
        "\n",
        "# model_apply_batch = jax.vmap(model.apply, in_axes=(None, 0), out_axes=(0))\n",
        "\n",
        "# *** new ***: Fuck, jax.jit makes it so much faster even on GPU.\n",
        "model_apply_batch = jax.jit(jax.vmap(model.apply, in_axes=(None, 0), out_axes=(0)))\n",
        "\n",
        "def forward_pass(params, state, batch):\n",
        "  inputs, targets = batch\n",
        "  logits = state.apply_fn({\"params\": params}, inputs)\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
        "  loss = loss.mean()\n",
        "  return loss\n",
        "\n",
        "grad_fn = jax.value_and_grad(forward_pass, argnums=(0))  # differentiate wrt 0th pos argument.\n",
        "\n",
        "opt = optax.adam(learning_rate=0.001)\n",
        "state = train_state.TrainState.create(apply_fn=model_apply_batch, params=params, tx=opt)\n",
        "\n",
        "for epoch in range(1000):\n",
        "  batch = get_batch()\n",
        "  loss, grads = grad_fn(state.params, state, batch)\n",
        "  print(loss) if epoch%100==0 else None\n",
        "  state = state.apply_gradients(grads=grads)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3i62quMPv4d",
        "outputId": "1ccc9212-1914-46ce-9e7b-09384a5e9823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.16981\n",
            "3.848041\n",
            "3.2327182\n",
            "2.9303553\n",
            "3.073019\n",
            "2.7059855\n",
            "2.8082283\n",
            "2.8490913\n",
            "2.7219577\n",
            "2.7902007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NanoGPT"
      ],
      "metadata": {
        "id": "-_JsJ5hoKQ-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  output_size: int\n",
        "\n",
        "  def setup(self):\n",
        "    super().setup()\n",
        "    # **new**: attention paper uses 4 times token_info_size when doing linear transformation.\n",
        "    # and then projects it back to token_info_size in linear transformation layer.\n",
        "    self.ffwd = nn.Dense(features=4 * self.output_size)\n",
        "\n",
        "    # **new**: projection layer, which goes back into residual pathway.\n",
        "    self.projection = nn.Dense(self.output_size)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = nn.relu(self.ffwd(x))\n",
        "    x = self.projection(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "M5XnQBReV27M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (copy-pasting single head attention from above)\n",
        "class Head(nn.Module):\n",
        "  token_info_size: int # head_size; how much (emb dim) info each token emits for keys, queries, values.\n",
        "  T: int # block size; number of tokens in a block\n",
        "\n",
        "  def setup(self):\n",
        "    super().setup()\n",
        "    # key, query will take vector of size C.\n",
        "    # i.e., channels containing info of token and will output token_info_size\n",
        "    self.key_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "    self.query_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "    self.value_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "\n",
        "\n",
        "  def __call__(self, block_of_tokens_with_info_channels: jnp.array):\n",
        "    \"\"\"Accepts a block of tokens with info channels, like (8, 65).\"\"\"\n",
        "\n",
        "    # TODO(ntnsonti): Double check; but tril should not be learnable according cGPT.\n",
        "    tril = jnp.tril(jnp.ones(shape=(self.T, self.T)))\n",
        "\n",
        "    keys = self.key_layer(block_of_tokens_with_info_channels) # (T, token_info_size)\n",
        "    queries = self.query_layer(block_of_tokens_with_info_channels)\n",
        "    values = self.value_layer(block_of_tokens_with_info_channels)\n",
        "\n",
        "    # chanel info size\n",
        "    C = int(block_of_tokens_with_info_channels.shape[-1])\n",
        "    print(\"[ntn99] channel_info_size: \", C)\n",
        "\n",
        "    # compute attention score.\n",
        "    wei = jnp.dot(queries, keys.T) * C**0.5 # (T, T)\n",
        "    wei = jnp.where(tril==0, -jnp.inf, wei)\n",
        "    wei = nn.softmax(wei, axis=-1)\n",
        "\n",
        "\n",
        "    out = jnp.dot(wei, values) # (T, T) * (T, token_info_size))\n",
        "    return out # (T, token_info_size)\n"
      ],
      "metadata": {
        "id": "gDjiKK7qQh7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  final_token_info_size: int # After concatenating from all heads, how much info (values -- emb size) you have on each token.\n",
        "  T: int\n",
        "\n",
        "  def setup(self):\n",
        "    super().setup()\n",
        "    self.token_info_size_per_head = int(self.final_token_info_size/self.num_heads)\n",
        "    self.heads = [\n",
        "        Head(token_info_size=self.token_info_size_per_head, T=self.T) for _ in range(self.num_heads)\n",
        "    ]\n",
        "\n",
        "    self.projection = nn.Dense(features=self.final_token_info_size)\n",
        "\n",
        "  def __call__(self, block_of_tokens_with_info_channels: jnp.array):\n",
        "    out_from_each_head = jnp.array([h(block_of_tokens_with_info_channels) for h in self.heads])\n",
        "\n",
        "    # You just run multiple attention heads in parallel and concatenate\n",
        "    # their output along channel dimension, i.e., dim==-1\n",
        "    out_from_all_heads = jnp.concatenate(out_from_each_head, axis=-1)\n",
        "    print(\"[ntn99] out_from_all_heads concatenated shape: \", out_from_all_heads.shape)\n",
        "\n",
        "    return self.projection(out_from_all_heads)"
      ],
      "metadata": {
        "id": "k8SK5oL4Qh7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  num_heads: int\n",
        "  final_token_info_size: int\n",
        "  T: int\n",
        "\n",
        "  def setup(self):\n",
        "    super().setup()\n",
        "\n",
        "    # communication.\n",
        "    self.self_attention_heads = MultiHeadAttention(num_heads=self.num_heads,\n",
        "                                                   final_token_info_size=self.final_token_info_size,\n",
        "                                                   T=self.T)\n",
        "\n",
        "    # computation.\n",
        "    self.computation_layer = FeedForward(output_size=self.final_token_info_size)\n",
        "\n",
        "    self.ln1 = nn.LayerNorm()\n",
        "    self.ln2 = nn.LayerNorm()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = x + self.self_attention_heads(self.ln1(x))\n",
        "    print(\"[ntn99] input size after attention_head: \", x.shape)\n",
        "\n",
        "    x = x + self.computation_layer(self.ln2(x))\n",
        "    print(\"[ntn99] input size after computation (end of block): \", x.shape)\n",
        "    return x"
      ],
      "metadata": {
        "id": "-uqusnxtXfEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  \"\"\"Reads one char and predicits the next char.\"\"\"\n",
        "  vocab_size: int # number of vocabulary (number of rows of embedding table)\n",
        "  n_embed: int # embedding dim after lookup\n",
        "  T: int # block size, i.e., number of tokens attention block is looking at once\n",
        "\n",
        "  def setup(self):\n",
        "    super().setup()\n",
        "    # number of channels you want to use for store info for each token.\n",
        "    self.C = self.vocab_size\n",
        "\n",
        "    self.token_embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.n_embed)\n",
        "\n",
        "    self.pos_embedding_table = nn.Embed(num_embeddings=self.T, features=self.n_embed)\n",
        "\n",
        "    # *** new ***\n",
        "    # Since, there are 4 heads, each head only needs to output token_info of size 8.\n",
        "    # Concantenate token_info from all 4 heards, gives us 32\n",
        "    # self.self_attention_heads = MultiHeadAttention(num_heads=4, final_token_info_size=self.n_embed, T=self.T)\n",
        "    self.blocks = nn.Sequential([\n",
        "        Block(num_heads=4, final_token_info_size=self.n_embed, T=self.T),\n",
        "        Block(num_heads=4, final_token_info_size=self.n_embed, T=self.T),\n",
        "        Block(num_heads=4, final_token_info_size=self.n_embed, T=self.T),\n",
        "        Block(num_heads=4, final_token_info_size=self.n_embed, T=self.T),\n",
        "        nn.LayerNorm(), # TODO: I think my reduction_axis should be 0.\n",
        "    ])\n",
        "\n",
        "    self.lang_model_head = nn.Dense(features=self.C)\n",
        "\n",
        "  def __call__(self, block_of_tokens: jnp.array):\n",
        "    \"\"\"Accepts a block of tokens, like [0, 1, 2, 3, 4, 5, 6, 7].\"\"\"\n",
        "\n",
        "    # generate em for each token. output: (T, n_embed)\n",
        "    token_embs = self.token_embedding_table(block_of_tokens)\n",
        "\n",
        "    # generate position embs for each token.\n",
        "    ## get token positions.\n",
        "    num_pos = block_of_tokens.shape[0]\n",
        "    positions = jnp.arange(0, num_pos)\n",
        "    pos_embs = self.pos_embedding_table(positions)\n",
        "\n",
        "    # generate actual input to attention, x, which is sum of token_embs + pos_embs\n",
        "    x = token_embs + pos_embs\n",
        "\n",
        "    # feed x into self-attention head.\n",
        "    # x = self.self_attention_heads(x)\n",
        "    x = self.blocks(x)\n",
        "\n",
        "    # generate logits for each token. output: (T, channels for info -- C)\n",
        "    token_logits = self.lang_model_head(x)\n",
        "\n",
        "    return token_logits\n"
      ],
      "metadata": {
        "id": "PmX47dBOQh7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=65, n_embed=32, T=BLOCK_SIZE)\n",
        "\n",
        "# Now, our language model needs to accept a block of tokens, not one-char at a time.\n",
        "# We'll then make it accept a batch of blocks of tokens using vmap.\n",
        "sample_block_of_tokens = jnp.ones(shape=(T), dtype=jnp.int32)\n",
        "output, params = model.init_with_output(jrand.PRNGKey(99), sample_block_of_tokens)\n",
        "params = params[\"params\"]\n",
        "\n",
        "# model_apply_batch = jax.vmap(model.apply, in_axes=(None, 0), out_axes=(0))\n",
        "\n",
        "# *** new ***: Fuck, jax.jit makes it so much faster even on GPU.\n",
        "model_apply_batch = jax.jit(jax.vmap(model.apply, in_axes=(None, 0), out_axes=(0)))\n",
        "\n",
        "def forward_pass(params, state, batch):\n",
        "  inputs, targets = batch\n",
        "  logits = state.apply_fn({\"params\": params}, inputs)\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
        "  loss = loss.mean()\n",
        "  return loss\n",
        "\n",
        "grad_fn = jax.value_and_grad(forward_pass, argnums=(0))  # differentiate wrt 0th pos argument.\n",
        "\n",
        "opt = optax.adam(learning_rate=0.0001)\n",
        "state = train_state.TrainState.create(apply_fn=model_apply_batch, params=params, tx=opt)\n",
        "\n",
        "for epoch in range(5000):\n",
        "  batch = get_batch()\n",
        "  loss, grads = grad_fn(state.params, state, batch)\n",
        "  print(\"loss\", loss, \"epoch\", epoch) if epoch%100==0 else None\n",
        "  state = state.apply_gradients(grads=grads)"
      ],
      "metadata": {
        "id": "5Bt4gmGPQh7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating\n",
        "for generating, I think we give block of tokens and then get next token prediction at each token. But you only uses the next token prediction of the last token because that has attended to everything before it.\n",
        "\n",
        "You just take the logit at last Tth position and you should pass everything but the 0th from previous tokens and then next predicted token."
      ],
      "metadata": {
        "id": "o_na8gjRleFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE, BLOCK_SIZE"
      ],
      "metadata": {
        "id": "y0pZ8QL4zwwE",
        "outputId": "a751d7b2-419f-40e8-88cf-5e8f9ac933f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T = BLOCK_SIZE\n",
        "T"
      ],
      "metadata": {
        "id": "wlJYHEdw0Jls",
        "outputId": "1196037f-77fc-4a05-db89-d9f7007d8e35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_apply_jit = jax.jit(state.apply_fn)\n",
        "state_apply_jit({\"params\": state.params}, context[:, -T:])"
      ],
      "metadata": {
        "id": "TVAifA_K13_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_token_logit.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaN73uy31JYD",
        "outputId": "af9e2d93-fea9-44ee-bbd6-fd7fff1a00aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 8, 65)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_token_logit[:, -1, :].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI8NJPTV1PHX",
        "outputId": "05ee0a84-7922-4a3d-852a-4ce9e0af2510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 65)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jax.random.categorical(split_key, next_token_logit[:, -1, :], axis=-1, shape=(1, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab_TTRyh1dFT",
        "outputId": "f5c55f24-f32e-4fe8-e6aa-180b4bb682f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[46]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## putting it together"
      ],
      "metadata": {
        "id": "rW_gYfOO2AjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = BLOCK_SIZE\n",
        "\n",
        "state_apply_jit = jax.jit(state.apply_fn)\n",
        "\n",
        "context = jnp.tile(jnp.array([52], dtype=jnp.int32), T)\n",
        "context = context[None, -T:]\n",
        "key = jrand.PRNGKey(99)\n",
        "\n",
        "for _ in range(100):\n",
        "  next_token_logits = state_apply_jit({\"params\": state.params}, context[:, -T:])\n",
        "\n",
        "  key, split_key = jrand.split(key)\n",
        "  new_token = jax.random.categorical(key, next_token_logits[:, -1, :], axis=-1, shape=(1, 1))\n",
        "\n",
        "  context = jnp.concatenate([context, new_token], axis=1)\n"
      ],
      "metadata": {
        "id": "RDlnp-9eoWJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode(context.tolist()[0], itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XoxECiyV3j9I",
        "outputId": "8add9577-7f16-48b6-840a-99e77af86bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nnnnnnnnp eSs ,orr\\nine, Co,R eaaotewl\\nNshM tuhk e \\nos,sitdN ci,Bf iC Oot d:y oWtBthrh l sre \\naonsoa\\noK, aAth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining all code in one cell\n",
        "*********"
      ],
      "metadata": {
        "id": "Pslr-91u__zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = BLOCK_SIZE"
      ],
      "metadata": {
        "id": "kUNTqRI-zud4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  output_size: int\n",
        "\n",
        "  def setup(self):\n",
        "    # **new**: attention paper uses 4 times token_info_size when doing linear transformation.\n",
        "    # and then projects it back to token_info_size in linear transformation layer.\n",
        "    self.ffwd = nn.Dense(features=4 * self.output_size)\n",
        "\n",
        "    # **new**: projection layer, which goes back into residual pathway.\n",
        "    self.projection = nn.Dense(self.output_size)\n",
        "\n",
        "  def __call__(self, x, training: bool):\n",
        "    x = nn.relu(self.ffwd(x))\n",
        "    x = self.projection(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "  token_info_size: int # head_size; how much (emb dim) info each token emits for keys, queries, values.\n",
        "  T: int # block size; number of tokens in a block\n",
        "\n",
        "  def setup(self):\n",
        "    # key, query will take vector of size C.\n",
        "    # i.e., channels containing info of token and will output token_info_size\n",
        "    self.key_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "    self.query_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "    self.value_layer = nn.Dense(self.token_info_size, use_bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(rate=0.2)\n",
        "\n",
        "\n",
        "  def __call__(self, block_of_tokens_with_info_channels: jnp.array, training: bool):\n",
        "    \"\"\"Accepts a block of tokens with info channels, like (8, 65).\"\"\"\n",
        "\n",
        "    # TODO(ntnsonti): Double check; but tril should not be learnable according cGPT.\n",
        "    tril = jnp.tril(jnp.ones(shape=(self.T, self.T)))\n",
        "\n",
        "    # input: (T, info channels )\n",
        "    # output: (T, token_info_size)\n",
        "    keys = self.key_layer(block_of_tokens_with_info_channels)\n",
        "    queries = self.query_layer(block_of_tokens_with_info_channels)\n",
        "    values = self.value_layer(block_of_tokens_with_info_channels)\n",
        "\n",
        "    # chanel info size\n",
        "    C = int(block_of_tokens_with_info_channels.shape[-1])\n",
        "    # print(\"[ntn99] channel_info_size: \", C)\n",
        "\n",
        "    # compute attention score.\n",
        "    wei = jnp.dot(queries, keys.T) * C**0.5 # (T, token_info_size) * (token_info_size, T) == (T, T)\n",
        "    wei = jnp.where(tril==0, -jnp.inf, wei)\n",
        "    wei = nn.softmax(wei, axis=-1)\n",
        "\n",
        "    attention_values = jnp.dot(wei, values) # (T, T) * (T, token_info_size))\n",
        "\n",
        "    attention_values = self.dropout(attention_values, deterministic=not training)\n",
        "\n",
        "    return attention_values # (T, token_info_size)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  final_token_info_size: int # After concatenating from all heads, how much info (values -- emb size) you have on each token.\n",
        "  T: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.token_info_size_per_head = int(self.final_token_info_size/self.num_heads)\n",
        "    self.heads = [\n",
        "        Head(token_info_size=self.token_info_size_per_head, T=self.T) for _ in range(self.num_heads)\n",
        "    ]\n",
        "\n",
        "    self.projection = nn.Dense(features=self.final_token_info_size)\n",
        "\n",
        "  def __call__(self, block_of_tokens_with_info_channels: jnp.array, training: bool):\n",
        "    out_from_each_head = jnp.array([h(block_of_tokens_with_info_channels, training) for h in self.heads])\n",
        "\n",
        "    # You just run multiple attention heads in parallel and concatenate\n",
        "    # their output along channel dimension, i.e., dim==-1\n",
        "    out_from_all_heads = jnp.concatenate(out_from_each_head, axis=-1)\n",
        "    # print(\"[ntn99] out_from_all_heads concatenated shape: \", out_from_all_heads.shape)\n",
        "\n",
        "    return self.projection(out_from_all_heads)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  num_heads: int\n",
        "  final_token_info_size: int\n",
        "  T: int\n",
        "\n",
        "  def setup(self):\n",
        "    # communication.\n",
        "    self.self_attention_heads = MultiHeadAttention(num_heads=self.num_heads,\n",
        "                                                   final_token_info_size=self.final_token_info_size,\n",
        "                                                   T=self.T)\n",
        "\n",
        "    # computation.\n",
        "    self.computation_layer = FeedForward(output_size=self.final_token_info_size)\n",
        "\n",
        "    self.ln1 = nn.LayerNorm()\n",
        "    self.ln2 = nn.LayerNorm()\n",
        "\n",
        "  def __call__(self, x, training: bool):\n",
        "    x = x + self.self_attention_heads(self.ln1(x), training)\n",
        "    # print(\"[ntn99] input size after attention_head: \", x.shape)\n",
        "\n",
        "    x = x + self.computation_layer(self.ln2(x), training)\n",
        "    # print(\"[ntn99] input size after computation (end of block): \", x.shape)\n",
        "    return x\n",
        "\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "  \"\"\"Reads one char and predicits the next char.\"\"\"\n",
        "  vocab_size: int # number of vocabulary (number of rows of embedding table)\n",
        "  n_embed: int # embedding dim after lookup\n",
        "  T: int # block size, i.e., number of tokens attention block is looking at once\n",
        "\n",
        "  def setup(self):\n",
        "    # number of channels you want to use for store info for each token.\n",
        "    self.C = self.vocab_size\n",
        "\n",
        "    self.token_embedding_table = nn.Embed(num_embeddings=self.vocab_size, features=self.n_embed)\n",
        "\n",
        "    self.pos_embedding_table = nn.Embed(num_embeddings=self.T, features=self.n_embed)\n",
        "\n",
        "    # *** new ***\n",
        "    # Since, there are 4 heads, each head only needs to output token_info of size 8.\n",
        "    # Concantenate token_info from all 4 heards, gives us 32\n",
        "    # self.self_attention_heads = MultiHeadAttention(num_heads=4, final_token_info_size=self.n_embed, T=self.T)\n",
        "    self.num_blocks = 4\n",
        "    self.blocks = [\n",
        "        Block(num_heads=4, final_token_info_size=self.n_embed, T=self.T) for _ in range(self.num_blocks)\n",
        "    ]\n",
        "    self.ln = nn.LayerNorm()\n",
        "    self.lang_model_head = nn.Dense(features=self.C)\n",
        "\n",
        "  def __call__(self, block_of_tokens: jnp.array, training: bool):\n",
        "    \"\"\"Accepts a block of tokens, like [0, 1, 2, 3, 4, 5, 6, 7].\"\"\"\n",
        "\n",
        "    # generate em for each token. output: (T, n_embed)\n",
        "    token_embs = self.token_embedding_table(block_of_tokens)\n",
        "\n",
        "    # generate position embs for each token.\n",
        "    ## get token positions.\n",
        "\n",
        "    # TODO(ntnsonti): setting num_pos to T always\n",
        "    # num_pos = block_of_tokens.shape[0]\n",
        "    num_pos = T\n",
        "    positions = jnp.arange(0, num_pos)\n",
        "    pos_embs = self.pos_embedding_table(positions)\n",
        "\n",
        "    # generate actual input to attention, x, which is sum of token_embs + pos_embs\n",
        "    x = token_embs + pos_embs\n",
        "\n",
        "    # feed x into self-attention head.\n",
        "    # x = self.self_attention_heads(x)\n",
        "    # x = self.blocks(x)(training)\n",
        "    for i in range(self.num_blocks):\n",
        "      x = self.blocks[i](x, training)\n",
        "\n",
        "    x = self.ln(x)\n",
        "\n",
        "    # generate logits for each token. output: (T, channels for info -- C)\n",
        "    token_logits = self.lang_model_head(x)\n",
        "\n",
        "    return token_logits\n"
      ],
      "metadata": {
        "id": "CYNdkmYcADtK"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training loop"
      ],
      "metadata": {
        "id": "Puz1V0xEAaYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainState(train_state.TrainState):\n",
        "  key: jax.random.KeyArray\n",
        "\n",
        "random_key = jax.random.PRNGKey(99)\n",
        "random_key, random_subkey = jax.random.split(random_key)\n",
        "\n",
        "model = LanguageModel(vocab_size=65, n_embed=32, T=BLOCK_SIZE)\n",
        "\n",
        "# Now, our language model needs to accept a block of tokens, not one-char at a time.\n",
        "# We'll then make it accept a batch of blocks of tokens using vmap.\n",
        "sample_block_of_tokens = jnp.ones(shape=(T), dtype=jnp.int32)\n",
        "output, params = model.init_with_output(jrand.PRNGKey(99), sample_block_of_tokens, training=False)\n",
        "params = params[\"params\"]\n"
      ],
      "metadata": {
        "id": "cYx3x2eyFVyo",
        "outputId": "c00d179a-04fa-4f13-94b6-39ff1a0e99ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-80ea40c9ea42>:2: DeprecationWarning: jax.random.KeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys (i.e. keys created with jax.random.key).\n",
            "For more information, see https://jax.readthedocs.io/en/latest/jep/9263-typed-keys.html\n",
            "  key: jax.random.KeyArray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2cU8qRc2JVUj",
        "outputId": "986f67d5-cf3d-4f9a-f01e-27f4e6a8713d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_apply_batch = jax.vmap(model.apply, in_axes=(None, 0, None), out_axes=(0))\n",
        "\n",
        "# *** new ***: Fuck, jax.jit makes it so much faster even on GPU.\n",
        "# model_apply_batch = jax.vmap(model.apply, in_axes=(None, 0), out_axes=(0))\n",
        "\n",
        "def forward_pass(params, state, batch, dropout_key):\n",
        "  inputs, targets = batch\n",
        "  logits = state.apply_fn({\"params\": params}, inputs, jnp.tile(jnp.array([True], dtype=jnp.int32), T), rngs={'dropout': dropout_key})\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
        "  loss = loss.mean()\n",
        "  return loss\n",
        "\n",
        "grad_fn = jax.value_and_grad(forward_pass, argnums=(0))  # differentiate wrt 0th pos argument.\n",
        "\n",
        "opt = optax.adam(learning_rate=0.0001)\n",
        "state = TrainState.create(apply_fn=model_apply_batch, params=params, tx=opt, key=random_key)\n",
        "\n",
        "for epoch in range(1):\n",
        "  batch = get_batch()\n",
        "\n",
        "  random_key, random_subkey = jax.random.split(random_key)\n",
        "  dropout_key = jax.random.fold_in(key=random_key, data=state.step)\n",
        "\n",
        "  loss, grads = grad_fn(state.params, state, batch, dropout_key)\n",
        "  print(\"loss\", loss, \"epoch\", epoch) if epoch%100==0 else None\n",
        "  state = state.apply_gradients(grads=grads)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "Y9MIDaZLAWKE",
        "outputId": "cd76d41f-b6cd-4cae-ebb0-02bc36d3db30"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "vmap got inconsistent sizes for array axes to be mapped:\n  * one axis had size 4: axis 0 of argument args[0] of type int32[4,8];\n  * one axis had size 2: axis 0 of argument rngs['dropout'] of type uint32[2]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-45ae903afb52>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mdropout_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-45ae903afb52>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(params, state, batch, dropout_key)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'dropout'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdropout_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_integer_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36m_mapped_axis_size\u001b[0;34m(fn, tree, vals, dims, name)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  * some axes ({ct} of them) had size {sz}, e.g. axis {ax} of {ex};\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove last semicolon and newline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: vmap got inconsistent sizes for array axes to be mapped:\n  * one axis had size 4: axis 0 of argument args[0] of type int32[4,8];\n  * one axis had size 2: axis 0 of argument rngs['dropout'] of type uint32[2]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generating"
      ],
      "metadata": {
        "id": "5z6RHHqQAc8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = BLOCK_SIZE\n",
        "\n",
        "state_apply_jit = jax.jit(state.apply_fn)\n",
        "\n",
        "context = jnp.tile(jnp.array([52], dtype=jnp.int32), T)\n",
        "context = context[None, -T:]\n",
        "key = jrand.PRNGKey(99)\n",
        "\n",
        "for _ in range(100):\n",
        "  next_token_logits = state_apply_jit({\"params\": state.params}, context[:, -T:])\n",
        "\n",
        "  key, split_key = jrand.split(key)\n",
        "  new_token = jax.random.categorical(key, next_token_logits[:, -1, :], axis=-1, shape=(1, 1))\n",
        "\n",
        "  context = jnp.concatenate([context, new_token], axis=1)\n",
        "\n",
        "\n",
        "decode(context.tolist()[0], itos)\n"
      ],
      "metadata": {
        "id": "8mow1yC3AcS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}